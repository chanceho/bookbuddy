{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78a2f407-030f-42ba-8b3b-add892a607d6",
   "metadata": {},
   "source": [
    "# Recommendation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c2c5f-d2ef-40d9-b530-1b6888124c6e",
   "metadata": {},
   "source": [
    "*This Jupyter Notebook imports and combines the two datasets (for young adult and children), performs exploratory data analysis, and generates the output for the Recommendation using both collaborative and content-based filtering.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbe788f-a096-4dfb-91e4-32f77e8ff5f9",
   "metadata": {},
   "source": [
    "# Section 1: Data Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e298ff-6725-42fd-be73-622d2cac1272",
   "metadata": {},
   "source": [
    "## 1.1 Importing & Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05453a41-a2c6-4a4d-a658-17d5f0e823fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Jia Wei\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Jia Wei\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Jia Wei\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (2.2.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Jia Wei\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Jia Wei\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Jia Wei\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Jia Wei\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Jia Wei\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Jia Wei\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.2.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (75.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Jia Wei\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Jia Wei\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Jia Wei\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 41.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.6/12.8 MB 32.9 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 30.9 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Jia Wei\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Jia Wei\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Jia Wei\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.50.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (0.29.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Jia Wei\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Jia Wei\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Jia Wei\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: jupyter in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: notebook in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter) (7.3.2)\n",
      "Requirement already satisfied: jupyter-console in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter) (7.16.6)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter) (6.29.5)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter) (8.1.5)\n",
      "Requirement already satisfied: jupyterlab in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter) (4.3.5)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipykernel->jupyter) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipykernel->jupyter) (1.8.12)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipykernel->jupyter) (9.0.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipykernel->jupyter) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipykernel->jupyter) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipykernel->jupyter) (1.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipykernel->jupyter) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipykernel->jupyter) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipykernel->jupyter) (26.2.1)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipykernel->jupyter) (6.4.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipykernel->jupyter) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipywidgets->jupyter) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipywidgets->jupyter) (3.0.13)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-console->jupyter) (3.0.50)\n",
      "Requirement already satisfied: pygments in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-console->jupyter) (2.19.1)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyterlab->jupyter) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyterlab->jupyter) (0.28.1)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyterlab->jupyter) (3.1.5)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyterlab->jupyter) (2.2.5)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyterlab->jupyter) (2.15.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyterlab->jupyter) (2.27.3)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyterlab->jupyter) (0.2.4)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyterlab->jupyter) (75.8.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbconvert->jupyter) (4.13.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbconvert->jupyter) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbconvert->jupyter) (3.0.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbconvert->jupyter) (3.1.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbconvert->jupyter) (0.10.2)\n",
      "Requirement already satisfied: nbformat>=5.7 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbconvert->jupyter) (5.10.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbconvert->jupyter) (1.5.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (1.4.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.3.6)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (308)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (23.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.12.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.5.3)\n",
      "Requirement already satisfied: overrides>=5.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.21.1)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.0.15)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.0)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.17.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.10.0)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (4.23.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbformat>=5.7->nbconvert->jupyter) (2.21.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter) (0.2.13)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4->nbconvert->jupyter) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4->nbconvert->jupyter) (4.12.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter) (1.3.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (21.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.23.1)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.2.1)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (6.0.2)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel->jupyter) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter) (0.2.3)\n",
      "Requirement already satisfied: fqdn in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.0.0)\n",
      "Requirement already satisfied: uri-template in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (24.11.1)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.22)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\users\\jia wei\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.9.0.20241206)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Jia Wei\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Jia Wei\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Jia Wei\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas\n",
    "! pip install scikit-learn\n",
    "! pip install nltk\n",
    "! pip install spacy\n",
    "! python -m spacy download en_core_web_sm\n",
    "! pip install sentence-transformers\n",
    "! pip install requests jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "257ebb4f-9f79-4148-ba7f-927359b8a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import sklearn\n",
    "import nltk\n",
    "import spacy\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from nltk import pos_tag\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d44fbd0-5263-4d90-b281-d2e56cf79bba",
   "metadata": {},
   "source": [
    "## 1.2 Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01acaf0a-4e96-4628-8a87-72466276a34d",
   "metadata": {},
   "source": [
    "From goodreads, we are able to download three sets of data.\n",
    "\n",
    "1. The `books` dataset outlines associated metadata to a specific book. Things of interest here would be `book_id`, `title`, `average_rating`, `ratings_count`,`description`, `num_pages`, `popular_shelves`,`image_url`,`authors`.\n",
    "2. The `reviews` dataset consists of text reviews that may or may not be added after a rating. As the test reviews do not seem to be useful at this time, we will leave it out. We can consider the data here to scrape for genre.\n",
    "3. The `interactions` dataset indicates whether or not a specific user has read and rated a specific book. It consists of columns `user_id`, `book_id`, `is_read` and `ratings`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0eb1cf-21f2-4d88-bd06-02575b11d476",
   "metadata": {},
   "source": [
    "We have done so for two different age categories (`children` and `young_adult`), and will combine them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec0cd8-182b-493f-b650-9a21d40f1e9a",
   "metadata": {},
   "source": [
    "### Importing and Filtering Books Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "321976c3-8bf3-45b6-9938-1a09649ef16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_loading = time.time()\n",
    "columns_of_interest = ['book_id', 'title', 'average_rating', 'ratings_count','description', 'num_pages', 'popular_shelves','image_url','authors']\n",
    "json_files = ['goodreads_books_children.json', 'goodreads_books_young_adult.json']\n",
    "data = []\n",
    "\n",
    "for json_file in json_files:\n",
    "    with open(json_file, 'r') as file:\n",
    "        for line in file:\n",
    "            record = json.loads(line)\n",
    "            filtered_record = {key:record[key] for key in columns_of_interest}\n",
    "            data.append(filtered_record)\n",
    "\n",
    "books = pd.DataFrame(data)\n",
    "books['description_length'] = books['description'].apply(len)\n",
    "books = books[books['description_length'] != 0] #filtering empty descriptions\n",
    "books = books.drop('description_length', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6274b98f-0d36-484b-b944-cd4322072be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_of_interest = ['author_id', 'name']\n",
    "json_files = ['goodreads_book_authors.json']\n",
    "data = []\n",
    "\n",
    "for json_file in json_files:\n",
    "    with open(json_file, 'r') as file:\n",
    "        for line in file:\n",
    "            record = json.loads(line)\n",
    "            filtered_record = {key:record[key] for key in columns_of_interest}\n",
    "            data.append(filtered_record)\n",
    "            \n",
    "authors = pd.DataFrame(data)\n",
    "\n",
    "def get_name(author_id, authors = authors):\n",
    "    if author_id in authors['author_id'].values:\n",
    "        return authors.loc[authors['author_id'] == author_id, 'name'].values[0]\n",
    "    else:\n",
    "        return None\n",
    "end_loading = time.time()\n",
    "duration_loading_books = (end_loading - start_loading)/60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dc7fe3-da55-419b-bae6-19171179f210",
   "metadata": {},
   "source": [
    "### Importing and Filtering Interactions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0426a95-59e6-4cf8-8c64-f9f9db1a82d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_loading = time.time()\n",
    "columns_of_interest = ['user_id','book_id','is_read','rating']\n",
    "json_files = ['goodreads_interactions_children.json', 'goodreads_interactions_young_adult.json']\n",
    "data = []\n",
    "\n",
    "for json_file in json_files:\n",
    "    with open(json_file, 'r') as file:\n",
    "        for line in file:\n",
    "            record = json.loads(line)\n",
    "            filtered_record = {key:record[key] for key in columns_of_interest}\n",
    "            data.append(filtered_record)\n",
    "            \n",
    "interactions = pd.DataFrame(data)\n",
    "interactions = interactions[interactions['is_read'] != 0] #removing ratings by people who have not read the book\n",
    "\n",
    "end_loading = time.time()\n",
    "duration_loading_interactions = (end_loading - start_loading)/60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356021e4-ff96-4676-bcda-3eaae72ae93b",
   "metadata": {},
   "source": [
    "# Section 2: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ec713d-a2f3-4dc2-b74a-baf81633d018",
   "metadata": {},
   "source": [
    "There are a few ways we can consider hybridizing the approaches. We will now do the ensemble method, which generates two separate recommendation lists and then takes the intersection. The code below should generate 3 random books from the data, which will be used as a test set.\n",
    "\n",
    "Other methods we could consider include (1) weighted hybrid, where a content-based score and a collaborative filtering score is calculated and subsequently combined with a weighted average, or (2) switching hybrid, where content-based filtering is used when the user is new, or when a book has very few ratings, and collaborative filtering is used when a user / book has sufficient history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e68a4c2-e09e-4ac2-b232-a48f4b15c34c",
   "metadata": {},
   "source": [
    "## 2.1 Generating Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d47506-f8d9-4212-aef2-ac269d654984",
   "metadata": {},
   "source": [
    "### 2.1.1 Content-Based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36ccb43-6ba0-4bda-8370-2a085071aa46",
   "metadata": {},
   "source": [
    "For Content-Based Filtering, we use **TF-IDF** and **Cosine Similarity** as our core algorithms. \n",
    "\n",
    "**Term Frequency-Inverse Document Frequency (TF-IDF)** uses NLP to identify important words in the `description` attribute of the selected book by evaluating how frequently they appear, relative to the descriptions of all other books in the dataset. Once this is done, we sort the books by how similar they are using **cosine similarity**, which measures the angle between the two vectors (books). If they have a small angle, the books have similar `description` and is thus considered to be similar in content.\n",
    "\n",
    "A better way to imagine this would be if Book A and Book B both have descriptions that talk about \"magic\", \"spells\" and \"wizards\", they would have similar TF-IDF vectors, and thus high cosine similarity scores.\n",
    "\n",
    "We have also wrote two algorithms to detect genre and suitable age ranges, to identify books in the same genre and similar age ranges. Books in the same genre targeted at a similar age range would enjoy a boost in their similarity scores. The details of detecting genre and detecting age ranges are stipulated below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40001584-1909-4e19-b962-050f2bde4223",
   "metadata": {},
   "source": [
    "#### Detecting Genre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e6efe3-9d1c-4db6-ba29-ee7b653dac7c",
   "metadata": {},
   "source": [
    "To detect genre, we first:\n",
    "\n",
    "1. Extract genres from user-assigned shelves (most reliable signal)\n",
    "2. Apply multiple NLP techniques to analyze book description and title. This includes (1) TF-IDF analysis with genre-specific vocabulary and (2) Named entity recognition to identify genre-related entities\n",
    "3. Combine all signals with appropriate weights (shelf data > NLP)\n",
    "4. Return top genres that meet minimum confidence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "212dcb10-2e4b-4731-b6f8-dc66104f30df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_book_genre_with_advanced_nlp(book_data, genre_classifier=None, min_confidence=3, exclude_shelves=None):\n",
    "\n",
    "    if exclude_shelves is None:\n",
    "        exclude_shelves = get_default_excluded_shelves()\n",
    "\n",
    "    # Extract genres from structured shelf data\n",
    "    shelf_genres = extract_genres_from_shelves(book_data, get_genre_map(), exclude_shelves)\n",
    "    title = str(book_data.get('title', ''))\n",
    "    description = str(book_data.get('description', ''))\n",
    "\n",
    "    nlp_genres = {}\n",
    "    # Only perform NLP analysis if we have enough text\n",
    "    if len (shelf_genres) < 4 & len(description) > 500:\n",
    "        nlp_genres.update(analyze_with_tfidf(title, description))\n",
    "        nlp_genres.update(extract_named_entities(title, description))\n",
    "\n",
    "\n",
    "    # Combine all signals and apply minimum confidence threshold\n",
    "    final_genres = combine_all_genre_signals(shelf_genres, nlp_genres, min_confidence)\n",
    "    return final_genres[:3]\n",
    "\n",
    "def get_default_excluded_shelves():\n",
    "    #removes shelf names that aren't useful for genre classification\n",
    "    return {\n",
    "        'to-read', 'currently-reading', 'owned', 'default', \n",
    "        'favorites', 'books-i-own', 'ebook', 'kindle', \n",
    "        'library', 'audiobook', 'owned-books', 'to-buy', \n",
    "        'calibre', 're-read', 'unread', 'favourites', 'my-books'\n",
    "    }\n",
    "\n",
    "def get_genre_map():\n",
    "    #Dictionary mapping shelf keywords to standardized genre names\n",
    "    return {\n",
    "        'fantasy': 'Fantasy',\n",
    "        'sci-fi': 'Science Fiction',\n",
    "        'science-fiction': 'Science Fiction',\n",
    "        'mystery': 'Mystery/Thriller',\n",
    "        'thriller': 'Mystery/Thriller',\n",
    "        'romance': 'Romance',\n",
    "        'historical': 'Historical Fiction',\n",
    "        'history': 'History',\n",
    "        'horror': 'Horror',\n",
    "        'young-adult': 'Young Adult',\n",
    "        'ya': 'Young Adult',\n",
    "        'childrens': 'Children\\'s',\n",
    "        'children': 'Children\\'s',\n",
    "        'kids': 'Children\\'s',\n",
    "        'dystopian': 'Dystopian',\n",
    "        'classic': 'Classics',\n",
    "        'classics': 'Classics',\n",
    "        'biography': 'Biography/Memoir',\n",
    "        'memoir': 'Biography/Memoir',\n",
    "        'autobiography': 'Biography/Memoir',\n",
    "        'self-help': 'Self Help',\n",
    "        'business': 'Business',\n",
    "        'philosophy': 'Philosophy',\n",
    "        'psychology': 'Psychology',\n",
    "        'science': 'Science',\n",
    "        'poetry': 'Poetry',\n",
    "        'comic': 'Comics/Graphic Novels',\n",
    "        'graphic-novel': 'Comics/Graphic Novels',\n",
    "        'manga': 'Manga',\n",
    "        'cooking': 'Cooking/Food',\n",
    "        'cookbook': 'Cooking/Food',\n",
    "        'food': 'Cooking/Food',\n",
    "        'travel': 'Travel',\n",
    "        'religion': 'Religion/Spirituality',\n",
    "        'spirituality': 'Religion/Spirituality',\n",
    "        'art': 'Art/Photography',\n",
    "        'photography': 'Art/Photography',\n",
    "        'reference': 'Reference',\n",
    "        'textbook': 'Textbook/Education',\n",
    "        'education': 'Textbook/Education'\n",
    "    }\n",
    "\n",
    "def extract_genres_from_shelves(book_data, genre_map, exclude_shelves):\n",
    "    \n",
    "    #Extract genre information from book's popular shelves data by using shelf counts as confidence scores (more users shelving = higher confidence)\n",
    "    \n",
    "    shelf_genres = {}\n",
    "    \n",
    "    popular_shelves = book_data.get('popular_shelves', [])\n",
    "    if isinstance(popular_shelves, list) and popular_shelves:\n",
    "        for shelf in popular_shelves:\n",
    "            shelf_name = shelf.get('name', '').strip().lower()\n",
    "            shelf_count = int(shelf.get('count', 0))\n",
    "            \n",
    "            if shelf_name in exclude_shelves:\n",
    "                continue\n",
    "            \n",
    "            for keyword, genre_name in genre_map.items():\n",
    "                if keyword in shelf_name:\n",
    "                    if genre_name in shelf_genres:\n",
    "                        shelf_genres[genre_name] += shelf_count\n",
    "                    else:\n",
    "                        shelf_genres[genre_name] = shelf_count\n",
    "                    break\n",
    "    \n",
    "    return shelf_genres\n",
    "\n",
    "def preprocess_text(text, lemmatize=True):\n",
    "    \"\"\"\n",
    "    Preprocess text by removing special characters, lemmatizing, etc. We convert text to lowercase, remove URLs and HTML tags, remove non-alphabetic \n",
    "    characters, normalize whitespace and optionally lemmatize words (reduce to base form)\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        word_list = nltk.word_tokenize(text)\n",
    "        text = ' '.join([lemmatizer.lemmatize(word) for word in word_list])\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def load_nlp_models():\n",
    "    \n",
    "    #Load spaCy models required for named entity recognition\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    except:\n",
    "        try:\n",
    "            spacy.cli.download(\"en_core_web_sm\")\n",
    "            nlp = spacy.load(\"en_core_web_sm\")\n",
    "        except:\n",
    "            nlp = None\n",
    "            \n",
    "    return nlp\n",
    "\n",
    "\n",
    "def analyze_with_tfidf(title, description):\n",
    "    \"\"\"\n",
    "    Analyze book text using TF-IDF comparison against genre-specific vocabulary.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Define genre-specific keyword sets\n",
    "    2. Preprocess the book text (title + description)\n",
    "    3. Create TF-IDF vectors for genre keywords and book text\n",
    "    4. Calculate cosine similarity between book vector and each genre vector\n",
    "    5. Convert similarities to confidence scores and return top matches\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Define genre keyword sets\n",
    "        genre_keywords = {\n",
    "            'Fantasy': 'magic wizard dragon elf quest sword magical kingdom witch sorcery myth fantasy',\n",
    "            'Science Fiction': 'space alien future technology robot dystopian sci-fi futuristic planet spacecraft',\n",
    "            'Mystery/Thriller': 'murder detective crime case investigation killer suspense clue mystery conspiracy',\n",
    "            'Romance': 'love relationship passion romantic heart affair marriage emotion desire dating romance',\n",
    "            'Historical Fiction': 'century historical period king queen ancient war empire era medieval history',\n",
    "            'Horror': 'fear terror ghost scary monster supernatural haunt nightmare blood evil dark horror',\n",
    "            'Young Adult': 'teen school young coming-of-age adolescent teenage youth friendship high-school',\n",
    "            'Children\\'s': 'child kid young picture-book learning bedtime simple adventure colorful illustrated',\n",
    "            'Biography/Memoir': 'life autobiography personal real journey memoir experience story true figure',\n",
    "            'Self Help': 'improve success happiness guide advice life motivation habit inspiration growth',\n",
    "            'Business': 'market company entrepreneur success management leadership strategy finance career investment',\n",
    "            'Dystopian': 'dystopia future society control survival oppression rebellion totalitarian apocalyptic regime'\n",
    "        }\n",
    "        \n",
    "        # Preprocess text\n",
    "        processed_text = preprocess_text(f\"{title} {description}\")\n",
    "        \n",
    "        # Create TF-IDF vectorizer\n",
    "        vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "        \n",
    "        # Create corpus with genre keywords and the book text\n",
    "        corpus = list(genre_keywords.values())\n",
    "        corpus.append(processed_text)\n",
    "        \n",
    "        # Calculate TF-IDF\n",
    "        tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "        \n",
    "        # Calculate similarity between book and each genre\n",
    "        last_row_index = tfidf_matrix.shape[0] - 1\n",
    "        similarities = cosine_similarity(tfidf_matrix[last_row_index], tfidf_matrix[:-1])[0]\n",
    "        \n",
    "        # Map similarities to genres\n",
    "        genres = {}\n",
    "        for i, genre in enumerate(genre_keywords.keys()):\n",
    "            # Convert similarity scores to a more intuitive range (0-10)\n",
    "            score = int(similarities[i] * 10)\n",
    "            if score > 3:  # Only consider reasonable matches\n",
    "                genres[genre] = score\n",
    "                \n",
    "        return genres\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def extract_named_entities(title, description):\n",
    "    #Extract named entities from book text, use spaCy NLP model to process text and extract named entities before and mapping them to potential genres.\n",
    "    \n",
    "    try:\n",
    "        nlp = load_nlp_models()\n",
    "        if not nlp:\n",
    "            return {}\n",
    "            \n",
    "        # Process text with spaCy\n",
    "        doc = nlp(f\"{title} {description}\")\n",
    "        \n",
    "        # Extract entities\n",
    "        entities = [ent.text.lower() for ent in doc.ents]\n",
    "        \n",
    "        # Define entity-genre associations\n",
    "        entity_genre_map = {\n",
    "            'fantasy': ['magic', 'wizard', 'dragon', 'elf', 'fairy', 'kingdom', 'quest', 'sorcerer'],\n",
    "            'science fiction': ['space', 'planet', 'alien', 'robot', 'future', 'technology'],\n",
    "            'historical fiction': ['century', 'king', 'queen', 'empire', 'war', 'battle', 'medieval', 'ancient'],\n",
    "            'biography': ['life', 'biography', 'autobiography', 'memoir', 'president', 'politician', 'artist'],\n",
    "            'science': ['research', 'experiment', 'theory', 'physics', 'biology', 'chemistry', 'scientist'],\n",
    "            'religion': ['god', 'church', 'bible', 'faith', 'spiritual', 'religion', 'prayer']\n",
    "        }\n",
    "        \n",
    "        # Find genres based on entities\n",
    "        genres = {}\n",
    "        for entity in entities:\n",
    "            for genre, keywords in entity_genre_map.items():\n",
    "                if any(keyword in entity for keyword in keywords):\n",
    "                    standardized_genre = standardize_genre(genre)\n",
    "                    genres[standardized_genre] = genres.get(standardized_genre, 0) + 1\n",
    "                    \n",
    "        return genres\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "def standardize_genre(genre):\n",
    "    #Standardize genre names to a consistent format.\n",
    "    \n",
    "    genre_map = {\n",
    "        'fantasy': 'Fantasy',\n",
    "        'science fiction': 'Science Fiction',\n",
    "        'historical fiction': 'Historical Fiction',\n",
    "        'biography': 'Biography/Memoir',\n",
    "        'science': 'Science',\n",
    "        'religion': 'Religion/Spirituality'\n",
    "    }\n",
    "    return genre_map.get(genre.lower(), genre.title())\n",
    "\n",
    "\n",
    "def combine_all_genre_signals(shelf_genres, nlp_genres, min_confidence):\n",
    "    #Combine genre signals from different sources with appropriate weights.\n",
    "    \n",
    "    combined_scores = Counter()\n",
    "    \n",
    "    # Add shelf genres with highest weight (explicit human categorization)\n",
    "    for genre, score in shelf_genres.items():\n",
    "        combined_scores[genre] += score * 3\n",
    "    \n",
    "    # Add NLP-detected genres with medium weight\n",
    "    for genre, score in nlp_genres.items():\n",
    "        combined_scores[genre] += score * 2\n",
    "    \n",
    "    # Sort by final score\n",
    "    sorted_genres = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Filter by minimum confidence\n",
    "    final_genres = [genre for genre, score in sorted_genres if score >= min_confidence]\n",
    "    \n",
    "    # Fallback if no confident genres\n",
    "    if not final_genres and sorted_genres:\n",
    "        final_genres = [sorted_genres[0][0]]\n",
    "    \n",
    "    return final_genres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83346f8-7b97-4216-b398-83102a956bac",
   "metadata": {},
   "source": [
    "#### Detecting Suitable Age Range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeda18c-34ee-4774-9cc7-c3f6fd61601b",
   "metadata": {},
   "source": [
    "To detect age ranges, we first:\n",
    "\n",
    "1. Extract age ranges from `popular_shelves` (most reliable signal)\n",
    "2. Failing that, we estimate the targeted age range by approximating the difficulty of the book (1) using the number of pages `num_pages`, (2) complexity of language in the book's `title` and `description` and (3) analysis of possible themes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32103eca-8712-4eda-baea-a969fc7a3d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_age_range(book_data):\n",
    "    if book_data.get('popular_shelves') is None:\n",
    "        book_data['popular_shelves'] = []\n",
    "    \n",
    "    title = str(book_data.get('title', ''))\n",
    "    description = str(book_data.get('description', ''))\n",
    "    num_pages = book_data['num_pages']\n",
    "\n",
    "    try:\n",
    "        num_pages = int(book_data['num_pages'])\n",
    "    except (KeyError, ValueError, TypeError):\n",
    "        num_pages = 0\n",
    "    \n",
    "    age_scores = {\n",
    "        '0-5': 0,\n",
    "        '5-10': 0,\n",
    "        '10-15': 0,\n",
    "        '15+': 0\n",
    "    }\n",
    "    \n",
    "    title_lower = title.lower()\n",
    "    description_lower = description.lower()\n",
    "    \n",
    "    # Advanced NLP analysis of title and description\n",
    "    title_complexity = analyze_text_complexity(title)\n",
    "    desc_complexity = analyze_text_complexity(description)\n",
    "    \n",
    "    # Apply title complexity to age scores\n",
    "    if title_complexity < 0.3:\n",
    "        age_scores['0-5'] += 10\n",
    "        age_scores['5-10'] += 5\n",
    "    elif title_complexity < 0.5:\n",
    "        age_scores['5-10'] += 8\n",
    "        age_scores['0-5'] += 4\n",
    "    elif title_complexity < 0.7:\n",
    "        age_scores['10-15'] += 8\n",
    "        age_scores['5-10'] += 4\n",
    "    else:\n",
    "        age_scores['15+'] += 8\n",
    "        age_scores['10-15'] += 4\n",
    "    \n",
    "    # Apply description complexity to age scores\n",
    "    if desc_complexity < 0.3:\n",
    "        age_scores['0-5'] += 12\n",
    "        age_scores['5-10'] += 6\n",
    "    elif desc_complexity < 0.5:\n",
    "        age_scores['5-10'] += 10\n",
    "        age_scores['0-5'] += 5\n",
    "    elif desc_complexity < 0.7:\n",
    "        age_scores['10-15'] += 10\n",
    "        age_scores['5-10'] += 5\n",
    "    else:\n",
    "        age_scores['15+'] += 12\n",
    "        age_scores['10-15'] += 6\n",
    "    \n",
    "    # POS tag patterns analysis for age appropriateness\n",
    "    pos_patterns = analyze_pos_patterns(description)\n",
    "    for age_range, score in pos_patterns.items():\n",
    "        age_scores[age_range] += score\n",
    "    \n",
    "    board_book_terms = ['board book', 'bedtime', 'goodnight', 'naptime', 'toddler', 'baby', \n",
    "                        'alphabet', 'counting', 'colors', 'shapes', 'lullaby', 'nursery']\n",
    "    \n",
    "    if any(term in title_lower or term in description_lower for term in board_book_terms):\n",
    "        age_scores['0-5'] += 12\n",
    "        age_scores['5-10'] -= 5\n",
    "    \n",
    "    early_reader_terms = ['early reader', 'beginning reader', 'learn to read', 'level reader',\n",
    "                          'first reader', 'step into reading', 'i can read', 'reading level']\n",
    "    \n",
    "    if any(term in title_lower or term in description_lower for term in early_reader_terms):\n",
    "        age_scores['5-10'] += 12\n",
    "        age_scores['0-5'] -= 2\n",
    "    \n",
    "    grade_terms = {\n",
    "        '0-5': ['preschool', 'pre-k', 'kindergarten'],\n",
    "        '5-10': ['grade 1', 'grade 2', 'grade 3', 'grade 4', 'first grade', 'second grade', \n",
    "                'third grade', 'fourth grade', 'fifth grade', 'elementary'],\n",
    "        '10-15': ['grade 5', 'grade 6', 'grade 7', 'grade 8', 'middle school', 'middle-grade', \n",
    "                 'middle grade', 'tween'],\n",
    "        '15+': ['grade 9', 'grade 10', 'grade 11', 'grade 12', 'high school', 'teen', 'young adult',\n",
    "               'ya', 'college', 'university']\n",
    "    }\n",
    "    \n",
    "    for age_range, terms in grade_terms.items():\n",
    "        if any(term in title_lower or term in description_lower for term in terms):\n",
    "            age_scores[age_range] += 10\n",
    "    \n",
    "    if num_pages <= 32:\n",
    "        age_scores['0-5'] += 15\n",
    "        age_scores['5-10'] -= 3\n",
    "    elif 33 <= num_pages <= 48:\n",
    "        age_scores['0-5'] += 10\n",
    "        age_scores['5-10'] += 5\n",
    "    elif 49 <= num_pages <= 80:\n",
    "        age_scores['5-10'] += 12\n",
    "        age_scores['0-5'] -= 2\n",
    "    elif 81 <= num_pages <= 120:\n",
    "        age_scores['5-10'] += 8\n",
    "        age_scores['10-15'] += 4\n",
    "    elif 121 <= num_pages <= 200:\n",
    "        age_scores['10-15'] += 8\n",
    "        age_scores['5-10'] += 4\n",
    "    elif 201 <= num_pages <= 350:\n",
    "        age_scores['10-15'] += 10\n",
    "        age_scores['15+'] += 5\n",
    "    elif num_pages > 350:\n",
    "        age_scores['15+'] += 12\n",
    "        age_scores['10-15'] += 6\n",
    "\n",
    "    # Content theme analysis with increased weight for theme matches\n",
    "    content_themes = analyze_text_themes(title_lower, description_lower)\n",
    "    for age_range, score in content_themes.items():\n",
    "        age_scores[age_range] += score * 1.5\n",
    "    \n",
    "    # Shelf analysis\n",
    "    shelves = book_data.get('popular_shelves', [])\n",
    "    shelf_age_indicators = analyze_shelves_for_age(shelves)\n",
    "    for age_range, score in shelf_age_indicators.items():\n",
    "        age_scores[age_range] += score\n",
    "    \n",
    "    max_score = max(age_scores.values())\n",
    "    final_age_range = max(age_scores.items(), key=lambda x: x[1])[0]\n",
    "    \n",
    "    return final_age_range\n",
    "\n",
    "def analyze_text_complexity(text):\n",
    "    if not text or len(text) < 5:\n",
    "        return 0.5\n",
    "    \n",
    "    try:\n",
    "        sentences = sent_tokenize(text)\n",
    "        words = word_tokenize(text)\n",
    "        \n",
    "        if not sentences or not words:\n",
    "            return 0.5\n",
    "        \n",
    "        avg_sentence_length = len(words) / max(1, len(sentences))\n",
    "        avg_word_length = sum(len(word) for word in words if word.isalpha()) / max(1, len([w for w in words if w.isalpha()]))\n",
    "        \n",
    "        # Calculate lexical diversity (larger vocabulary suggests more complex text)\n",
    "        unique_words = len(set(word.lower() for word in words if word.isalpha()))\n",
    "        lexical_diversity = unique_words / max(1, len([w for w in words if w.isalpha()]))\n",
    "        \n",
    "        # Calculate percentage of complex words (words with 3+ syllables)\n",
    "        complex_words = sum(1 for word in words if word.isalpha() and textstat.syllable_count(word) >= 3)\n",
    "        complex_words_pct = complex_words / max(1, len([w for w in words if w.isalpha()]))\n",
    "        \n",
    "        # Weighted complexity score\n",
    "        complexity_score = (\n",
    "            (avg_sentence_length / 25) * 0.3 + \n",
    "            (avg_word_length / 7) * 0.2 + \n",
    "            lexical_diversity * 0.25 + \n",
    "            complex_words_pct * 0.25\n",
    "        )\n",
    "        \n",
    "        return min(1.0, complexity_score)\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "def analyze_pos_patterns(text):\n",
    "    try:\n",
    "        age_patterns = {\n",
    "            '0-5': 0,\n",
    "            '5-10': 0,\n",
    "            '10-15': 0,\n",
    "            '15+': 0\n",
    "        }\n",
    "        \n",
    "        # Get POS tags\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        tagged = pos_tag(tokens)\n",
    "        \n",
    "        # Count parts of speech\n",
    "        pos_counts = Counter(tag for word, tag in tagged)\n",
    "        total_tokens = len(tagged)\n",
    "        \n",
    "        if total_tokens == 0:\n",
    "            return age_patterns\n",
    "        \n",
    "        # Simple sentence structure (mainly nouns and verbs) - for young children\n",
    "        simple_structure = (pos_counts.get('NN', 0) + pos_counts.get('NNS', 0) + \n",
    "                           pos_counts.get('VB', 0) + pos_counts.get('VBZ', 0) + \n",
    "                           pos_counts.get('VBP', 0)) / total_tokens\n",
    "        \n",
    "        # Complex sentence markers (conjunctions, relative pronouns, etc.)\n",
    "        complex_markers = (pos_counts.get('IN', 0) + pos_counts.get('WDT', 0) + \n",
    "                          pos_counts.get('WP', 0) + pos_counts.get('WRB', 0)) / total_tokens\n",
    "        \n",
    "        # Advanced language features (adjectives, adverbs, etc.)\n",
    "        advanced_features = (pos_counts.get('JJ', 0) + pos_counts.get('JJR', 0) + \n",
    "                            pos_counts.get('JJS', 0) + pos_counts.get('RB', 0) + \n",
    "                            pos_counts.get('RBR', 0) + pos_counts.get('RBS', 0)) / total_tokens\n",
    "        \n",
    "        # Score assignment based on POS patterns\n",
    "        if simple_structure > 0.6 and complex_markers < 0.1:\n",
    "            age_patterns['0-5'] += 8\n",
    "            age_patterns['5-10'] += 4\n",
    "        elif simple_structure > 0.5 and complex_markers < 0.15:\n",
    "            age_patterns['5-10'] += 7\n",
    "            age_patterns['0-5'] += 3\n",
    "        elif complex_markers > 0.15 and advanced_features > 0.2:\n",
    "            age_patterns['10-15'] += 6\n",
    "            age_patterns['15+'] += 3\n",
    "        elif complex_markers > 0.2 and advanced_features > 0.25:\n",
    "            age_patterns['15+'] += 8\n",
    "            age_patterns['10-15'] += 4\n",
    "        \n",
    "        return age_patterns\n",
    "    except:\n",
    "        return {\n",
    "            '0-5': 0,\n",
    "            '5-10': 0,\n",
    "            '10-15': 0,\n",
    "            '15+': 0\n",
    "        }\n",
    "\n",
    "def analyze_text_themes(title, description):\n",
    "    combined_text = title + \" \" + description\n",
    "    \n",
    "    theme_scores = {\n",
    "        '0-5': 0,\n",
    "        '5-10': 0,\n",
    "        '10-15': 0,\n",
    "        '15+': 0\n",
    "    }\n",
    "    \n",
    "    early_themes = ['sleep', 'bed', 'nap', 'dream', 'moon', 'star', 'night', 'bunny', 'teddy', \n",
    "                    'toy', 'farm', 'animal', 'cat', 'dog', 'duck', 'color', 'zoo', 'mommy', \n",
    "                    'daddy', 'parent', 'bath', 'diaper', 'potty', 'train', 'truck', 'car', \n",
    "                    'alphabet', 'abc', 'number', '123', 'count', 'rhyme']\n",
    "    \n",
    "    elementary_themes = ['school', 'teacher', 'friend', 'adventure', 'fun', 'magic', 'fairy', \n",
    "                        'dragon', 'dinosaur', 'spy', 'detective', 'mystery', 'solve', 'game', \n",
    "                        'play', 'team', 'sport', 'chapter', 'series', 'collect', 'comic', \n",
    "                        'joke', 'funny', 'humor', 'silly', 'prank', 'robot', 'space', 'science']\n",
    "    \n",
    "    middle_themes = ['friend', 'school', 'bully', 'crush', 'team', 'competition', 'journal', \n",
    "                    'diary', 'secret', 'club', 'grow', 'family', 'sibling', 'parent', 'problem', \n",
    "                    'solve', 'quest', 'mission', 'summer', 'camp', 'vacation', 'holiday', \n",
    "                    'fantasy', 'world', 'magic', 'spell', 'creature', 'monster', 'ghost']\n",
    "    \n",
    "    ya_themes = ['love', 'romance', 'relationship', 'kiss', 'boyfriend', 'girlfriend', 'dating', \n",
    "                'death', 'tragedy', 'war', 'battle', 'fight', 'survive', 'future', 'dystopian', \n",
    "                'apocalypse', 'society', 'rebellion', 'government', 'power', 'politics', 'identity', \n",
    "                'struggle', 'college', 'career', 'adult', 'mature', 'violence', 'blood']\n",
    "    \n",
    "    for theme in early_themes:\n",
    "        if theme in combined_text:\n",
    "            theme_scores['0-5'] += 1.5\n",
    "    \n",
    "    for theme in elementary_themes:\n",
    "        if theme in combined_text:\n",
    "            theme_scores['5-10'] += 1.5\n",
    "    \n",
    "    for theme in middle_themes:\n",
    "        if theme in combined_text:\n",
    "            theme_scores['10-15'] += 1.5\n",
    "    \n",
    "    for theme in ya_themes:\n",
    "        if theme in combined_text:\n",
    "            theme_scores['15+'] += 1.5\n",
    "    \n",
    "    return theme_scores\n",
    "\n",
    "def analyze_shelves_for_age(shelves):\n",
    "    shelf_patterns = {\n",
    "        '0-5': ['picture book', 'board book', 'childrens', 'toddler', 'baby', 'preschool', \n",
    "               'bedtime', 'nursery', 'concept book'],\n",
    "        '5-10': ['early reader', 'chapter book', 'childrens', 'kids', 'elementary', 'juvenile', \n",
    "                'easy reader'],\n",
    "        '10-15': ['middle grade', 'middle-grade', 'tween', 'juvenile', 'preteen'],\n",
    "        '15+': ['young adult', 'ya', 'teen', 'high school', 'new adult', 'adult']\n",
    "    }\n",
    "    \n",
    "    shelf_scores = {\n",
    "        '0-5': 0,\n",
    "        '5-10': 0,\n",
    "        '10-15': 0,\n",
    "        '15+': 0\n",
    "    }\n",
    "    \n",
    "    for shelf in shelves:\n",
    "        shelf_name = shelf.get('name', '').lower()\n",
    "        shelf_count = int(shelf.get('count', 0))\n",
    "        \n",
    "        for age_range, patterns in shelf_patterns.items():\n",
    "            if any(pattern in shelf_name for pattern in patterns):\n",
    "                shelf_scores[age_range] += min(12, math.log(shelf_count + 1) * 2)\n",
    "    \n",
    "    return shelf_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee9f08a-9e7c-409c-ae00-d4dbd8810fe5",
   "metadata": {},
   "source": [
    "#### Combining Genre, Age and Description Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2bb9f67-96ea-4397-a332-7384fb9bb3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words = 'english') \n",
    "tfidf_matrix = tfidf.fit_transform(books['description']) #generating TF-IDF matrix\n",
    "\n",
    "def get_content_recommendations (book_id, df = books, tfidf_matrix = tfidf_matrix, top_n = 3):\n",
    "    book_row = df[df['book_id'] == book_id]\n",
    "    index = df.index.get_loc(book_row.index[0])\n",
    "    book_data = book_row.iloc[0].to_dict()\n",
    "    \n",
    "    target_genres = detect_book_genre_with_advanced_nlp(book_data)\n",
    "    target_age_range = detect_age_range(book_data)\n",
    "\n",
    "    sim_scores = cosine_similarity(tfidf_matrix[index], tfidf_matrix).flatten() #calculating cosine similarity\n",
    "\n",
    "    for i in range(len(sim_scores)):\n",
    "        if i == index:\n",
    "            continue\n",
    "        book_data_i = df.iloc[i].to_dict()\n",
    "        genres_i = detect_book_genre_with_advanced_nlp(book_data_i)\n",
    "        age_range_i = detect_age_range(book_data_i)\n",
    "\n",
    "        if set(target_genres) & set(genres_i):\n",
    "            sim_scores[i] *= 2\n",
    "        if set(target_age_range) & set(age_range_i):\n",
    "            sim_scores[i] *= 2        \n",
    "    \n",
    "    top_indices = np.argsort(sim_scores)[::1][1:top_n + 1]\n",
    "\n",
    "    recommendations = df.iloc[top_indices][['book_id']]\n",
    "\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79039930-ecdd-488e-b4aa-85b7752ee15e",
   "metadata": {},
   "source": [
    "### 2.1.2 Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855c2797-a584-4165-b923-c952be9e8c85",
   "metadata": {},
   "source": [
    "We will use K-Nearest Neighbours as an algorithm to perform Collaborative Filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78f0b03b-ec8c-4a0c-a2e2-c7c02f7dc704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_item_matrix(df): #to create user-item matrix for collaborative filtering\n",
    "    users = interactions['user_id'].nunique()\n",
    "    items = interactions['book_id'].nunique()\n",
    "    \n",
    "    user_mapper = dict(zip(np.unique(interactions['user_id']), list(range(users))))\n",
    "    user_inv_mapper = dict(zip(list(range(users)), np.unique(interactions['user_id'])))\n",
    "    user_index = [user_mapper[i] for i in interactions['user_id']]\n",
    "\n",
    "    \n",
    "    item_mapper = dict(zip(np.unique(interactions['book_id']), list(range(items))))\n",
    "    item_inv_mapper = dict(zip(list(range(items)), np.unique(interactions['book_id'])))\n",
    "    item_index = [item_mapper[i] for i in interactions['book_id']]\n",
    "    \n",
    "    user_item_matrix = csr_matrix((interactions['rating'], (user_index, item_index)), shape = (users, items))\n",
    "    return user_item_matrix, user_mapper, item_mapper, user_inv_mapper, item_inv_mapper\n",
    "\n",
    "user_item_matrix, user_mapper, item_mapper, user_inv_mapper, item_inv_mapper = create_user_item_matrix(interactions)\n",
    "\n",
    "def get_collaborative_recommendations(book_id, books = books, user_item_matrix = user_item_matrix, item_mapper = item_mapper, item_inv_mapper = item_inv_mapper, top_n = 3):\n",
    "    user_item_matrix = user_item_matrix.T\n",
    "    neighbor_ids = []\n",
    "    recommendations = []\n",
    "\n",
    "    item_ind = item_mapper[book_id]\n",
    "    item_vec = user_item_matrix[item_ind]\n",
    "    if isinstance(item_vec, (np.ndarray)):\n",
    "        item_vec = item_vec.reshape(1,-1)\n",
    "\n",
    "    kNN = NearestNeighbors(n_neighbors = top_n + 1, algorithm = \"brute\", metric = \"cosine\") #measuring similarity using K-Nearest-Neighbors\n",
    "    kNN.fit(user_item_matrix)\n",
    "    neighbor = kNN.kneighbors(item_vec, return_distance = False)\n",
    "    for i in range (0, top_n + 1):\n",
    "        n = neighbor.item(i)\n",
    "        neighbor_ids.append(item_inv_mapper[n])\n",
    "    neighbor_ids.pop(0)\n",
    "\n",
    "    for id in neighbor_ids: #retrieving book titles\n",
    "        recommended_books = books.loc[books['book_id'] == id, ['book_id','title']].values[0]\n",
    "        recommendations.append({'book_id': recommended_books[0], 'title': recommended_books[1]})\n",
    "        \n",
    "    recommendations_df = pd.DataFrame(recommendations)\n",
    "    return recommendations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b415c7-b2c8-445a-87c0-238cdc49762b",
   "metadata": {},
   "source": [
    "## 2.2 Parsing Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2ade622-a348-4d1e-9a61-8715594ad37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(book_id, books = books, authors = authors):\n",
    "    print(\"Fetching recommendations...\")\n",
    "    step0 = time.time()\n",
    "    df1 = get_content_recommendations(book_id)\n",
    "    step1 = time.time()\n",
    "    print(f\"Content recommendations loaded! Total time taken was {((step1 - step0) / 60):.4f} mins\")\n",
    "    df2 = get_collaborative_recommendations(book_id)\n",
    "    step2 = time.time()\n",
    "    print(f\"Collaborative recommendations loaded! Total time taken was {((step2 - step1) / 60):.4f} mins\")\n",
    "    recommendations = pd.concat([df1, df2], ignore_index = True)\n",
    "\n",
    "    result = []\n",
    "    print('Writing to File...')\n",
    "\n",
    "    for book_id in recommendations['book_id']:\n",
    "        book_details = books[books['book_id'] == book_id].iloc[0]\n",
    "        \n",
    "        authors_list = book_details['authors']\n",
    "        author_names = []\n",
    "        for author in authors_list:\n",
    "            author_id = author['author_id']\n",
    "            author_name = get_name(author_id) \n",
    "            author_names.append(author_name)\n",
    "\n",
    "        if len(author_names) > 1:\n",
    "            concat_authors = \" & \".join(author_names)\n",
    "        else:\n",
    "            concat_authors = author_names[0] if author_names else \"Unknown\"\n",
    "\n",
    "        book_metadata = {\n",
    "            \"bookid\": book_details['book_id'],\n",
    "            \"title\": book_details['title'],\n",
    "            \"author\": concat_authors,\n",
    "            \"coverimage\": book_details['image_url']\n",
    "        }\n",
    "\n",
    "        result.append(book_metadata)\n",
    "\n",
    "        with open('recommendations.json', 'w') as f:\n",
    "            json.dump(result, f, indent=4)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0058c569-2070-439b-9635-fca3c2a755a5",
   "metadata": {},
   "source": [
    "## 2.3 Testing with Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5715438-1e25-42c4-a4a1-e799d08d0e6d",
   "metadata": {},
   "source": [
    "To ensure the algorithm works, we randomly select 3 books and compute their recommendations list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7431271c-b629-4134-b029-5e979672513b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for loading books dataset: 0.1788 mins\n",
      "Time taken for loading interactions dataset: 2.3806 mins\n"
     ]
    }
   ],
   "source": [
    "print(f\"Time taken for loading books dataset: {duration_loading_books:.4f} mins\")\n",
    "print(f\"Time taken for loading interactions dataset: {duration_loading_interactions:.4f} mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "184a0c83-f597-44ee-af38-22a285ed06b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206456    16074780\n",
      "Name: book_id, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def generate_test_set(df, n = 1):\n",
    "    selected_books = df.sample(n)\n",
    "    return selected_books['book_id']\n",
    "test_set = generate_test_set(books)\n",
    "print(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4396a32-0813-4e82-8d3a-dd1465e462f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching recommendations...\n",
      "Content recommendations loaded! Total time taken was 5.0150 mins\n",
      "Collaborative recommendations loaded! Total time taken was 0.0252 mins\n",
      "Writing to File...\n",
      "[{'bookid': '18670062', 'title': 'Dream Dark (Caster Chronicles, #2.5)', 'author': 'Kami Garcia & Margaret Stohl', 'coverimage': 'https://images.gr-assets.com/books/1381774112m/18670062.jpg'}, {'bookid': '645967', 'title': 'Eggs Mark the Spot', 'author': 'Mary Jane Auch', 'coverimage': 'https://images.gr-assets.com/books/1298825696m/645967.jpg'}, {'bookid': '2834400', 'title': \"My Grandmother's Stories: A Collection of Jewish Folk Tales\", 'author': 'Adele Geras & Jael Jordan (Illustrator)', 'coverimage': 'https://s.gr-assets.com/assets/nophoto/book/111x148-bcc042a9c91a29c1d680899eff700a03.png'}, {'bookid': '24885888', 'title': 'The Boy I Love', 'author': 'Nina de Gramont', 'coverimage': 'https://images.gr-assets.com/books/1441586936m/24885888.jpg'}, {'bookid': '30313378', 'title': \"The You I've Never Known\", 'author': 'Ellen Hopkins', 'coverimage': 'https://images.gr-assets.com/books/1466958730m/30313378.jpg'}, {'bookid': '29654101', 'title': 'Heir to the Sky', 'author': 'Amanda Sun', 'coverimage': 'https://images.gr-assets.com/books/1464498155m/29654101.jpg'}]\n"
     ]
    }
   ],
   "source": [
    "for book in test_set:\n",
    "    recommendations = get_recommendations(book)\n",
    "    print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7dec83-d176-4125-a913-cd0c3a70e97c",
   "metadata": {},
   "source": [
    "# Section 3: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4698834-ab87-4ab0-aae4-76114577ad85",
   "metadata": {},
   "source": [
    "From exploratory data analysis, we deduce that the following list are the top 10 most rated books. We thus decide to evaluate the hybrid recommendation model using the Precision@6 metric. To do this, we used the top 10 most rated books, running the recommender for each test book to generate the 6 recommended titles. We then manually assessed how much of these 6 recommendations are genuinely relevant, and record that number. We then averaged across all ten books to get the overall precision score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3c363e1-81f6-452e-bce1-7e2e02742c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "books['book_id'] = books['book_id'].astype(str)\n",
    "test_books = [\"2767052\",\"41865\",\"5\",\"6148028\",\"11870085\",\"7260188\", \"13335037\",\"49041\",\"428263\",\"1162543\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f585046f-c970-4ff1-b9c6-3f020c13f8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching recommendations...\n",
      "Content recommendations loaded! Total time taken was 4.9561 mins\n",
      "Collaborative recommendations loaded! Total time taken was 0.0227 mins\n",
      "Writing to File...\n",
      "Fetching recommendations...\n",
      "Content recommendations loaded! Total time taken was 4.9032 mins\n",
      "Collaborative recommendations loaded! Total time taken was 0.0202 mins\n",
      "Writing to File...\n",
      "Fetching recommendations...\n",
      "Content recommendations loaded! Total time taken was 4.8549 mins\n",
      "Collaborative recommendations loaded! Total time taken was 0.0218 mins\n",
      "Writing to File...\n",
      "Fetching recommendations...\n",
      "Content recommendations loaded! Total time taken was 4.9250 mins\n",
      "Collaborative recommendations loaded! Total time taken was 0.0192 mins\n",
      "Writing to File...\n",
      "Fetching recommendations...\n",
      "Content recommendations loaded! Total time taken was 5.1777 mins\n",
      "Collaborative recommendations loaded! Total time taken was 0.0225 mins\n",
      "Writing to File...\n",
      "Fetching recommendations...\n",
      "Content recommendations loaded! Total time taken was 5.3918 mins\n",
      "Collaborative recommendations loaded! Total time taken was 0.0204 mins\n",
      "Writing to File...\n",
      "Fetching recommendations...\n",
      "Content recommendations loaded! Total time taken was 5.2050 mins\n",
      "Collaborative recommendations loaded! Total time taken was 0.0211 mins\n",
      "Writing to File...\n",
      "Fetching recommendations...\n",
      "Content recommendations loaded! Total time taken was 5.1832 mins\n",
      "Collaborative recommendations loaded! Total time taken was 0.0193 mins\n",
      "Writing to File...\n",
      "Fetching recommendations...\n",
      "Content recommendations loaded! Total time taken was 5.2219 mins\n",
      "Collaborative recommendations loaded! Total time taken was 0.0193 mins\n",
      "Writing to File...\n",
      "Fetching recommendations...\n",
      "Content recommendations loaded! Total time taken was 5.3538 mins\n",
      "Collaborative recommendations loaded! Total time taken was 0.0208 mins\n",
      "Writing to File...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_book_title</th>\n",
       "      <th>recommendation 1</th>\n",
       "      <th>recommendation 2</th>\n",
       "      <th>recommendation 3</th>\n",
       "      <th>recommendation 4</th>\n",
       "      <th>recommendation 5</th>\n",
       "      <th>recommendation 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Hunger Games (The Hunger Games, #1)</td>\n",
       "      <td>Being Me With OCD: How I Learned to Obsess Les...</td>\n",
       "      <td>Everwild (Skinjacker, #2)</td>\n",
       "      <td>Brave the Betrayal (Everworld, #8)</td>\n",
       "      <td>Catching Fire (The Hunger Games, #2)</td>\n",
       "      <td>Mockingjay (The Hunger Games, #3)</td>\n",
       "      <td>Twilight (Twilight, #1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twilight (Twilight, #1)</td>\n",
       "      <td>Peggy Sue Contra Los Invisibles</td>\n",
       "      <td>Supercomputer (Choose Your Own Adventure, #39)</td>\n",
       "      <td>Charlie Bone und das Geheimnis der sprechenden...</td>\n",
       "      <td>New Moon (Twilight, #2)</td>\n",
       "      <td>Eclipse (Twilight, #3)</td>\n",
       "      <td>Breaking Dawn (Twilight, #4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Prisoner of Azkaban (Harr...</td>\n",
       "      <td>Katso eteesi, Lotta!</td>\n",
       "      <td>The tale of Jemima puddle-duck</td>\n",
       "      <td>Sjælefanger (Riley Bloom, #1)</td>\n",
       "      <td>Catching Fire (The Hunger Games, #2)</td>\n",
       "      <td>The Hunger Games (The Hunger Games, #1)</td>\n",
       "      <td>Mockingjay (The Hunger Games, #3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Catching Fire (The Hunger Games, #2)</td>\n",
       "      <td>Alice's adventures in Wonderland ; and, Throug...</td>\n",
       "      <td>The Aeneid for Boys and Girls</td>\n",
       "      <td>Brave the Betrayal (Everworld, #8)</td>\n",
       "      <td>Mockingjay (The Hunger Games, #3)</td>\n",
       "      <td>The Hunger Games (The Hunger Games, #1)</td>\n",
       "      <td>Insurgent (Divergent, #2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Fault in Our Stars</td>\n",
       "      <td>Ondine (Ondine Quartet, #0.5)</td>\n",
       "      <td>Ölmem Gerekirse (Revenants, #3)</td>\n",
       "      <td>Itsy Bitsy Spider</td>\n",
       "      <td>Looking for Alaska</td>\n",
       "      <td>The Hunger Games (The Hunger Games, #1)</td>\n",
       "      <td>Divergent (Divergent, #1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mockingjay (The Hunger Games, #3)</td>\n",
       "      <td>Katso eteesi, Lotta!</td>\n",
       "      <td>Twinkle, Twinkle, Little Star</td>\n",
       "      <td>Itsy Bitsy Spider</td>\n",
       "      <td>Catching Fire (The Hunger Games, #2)</td>\n",
       "      <td>The Hunger Games (The Hunger Games, #1)</td>\n",
       "      <td>Insurgent (Divergent, #2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Divergent (Divergent, #1)</td>\n",
       "      <td>Kiki and Roo</td>\n",
       "      <td>Bloß nicht blinzeln!</td>\n",
       "      <td>The Princess and Curdie</td>\n",
       "      <td>Insurgent (Divergent, #2)</td>\n",
       "      <td>The Hunger Games (The Hunger Games, #1)</td>\n",
       "      <td>Allegiant (Divergent, #3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>New Moon (Twilight, #2)</td>\n",
       "      <td>Fear Hall: The Conclusion</td>\n",
       "      <td>Sit Still!</td>\n",
       "      <td>Einstein the Class Hamster and the Very Real G...</td>\n",
       "      <td>Eclipse (Twilight, #3)</td>\n",
       "      <td>Breaking Dawn (Twilight, #4)</td>\n",
       "      <td>Twilight (Twilight, #1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Eclipse (Twilight, #3)</td>\n",
       "      <td>Insurgent (Divergent, #2)</td>\n",
       "      <td>Half bad - Det mörka ödet (The Half Bad Trilog...</td>\n",
       "      <td>The Island</td>\n",
       "      <td>New Moon (Twilight, #2)</td>\n",
       "      <td>Breaking Dawn (Twilight, #4)</td>\n",
       "      <td>Twilight (Twilight, #1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Breaking Dawn (Twilight, #4)</td>\n",
       "      <td>Sun Up</td>\n",
       "      <td>The Mystery of the Burnt Cottage (The Five Fin...</td>\n",
       "      <td>Woof!</td>\n",
       "      <td>Eclipse (Twilight, #3)</td>\n",
       "      <td>New Moon (Twilight, #2)</td>\n",
       "      <td>Twilight (Twilight, #1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     test_book_title  \\\n",
       "0            The Hunger Games (The Hunger Games, #1)   \n",
       "1                            Twilight (Twilight, #1)   \n",
       "2  Harry Potter and the Prisoner of Azkaban (Harr...   \n",
       "3               Catching Fire (The Hunger Games, #2)   \n",
       "4                             The Fault in Our Stars   \n",
       "5                  Mockingjay (The Hunger Games, #3)   \n",
       "6                          Divergent (Divergent, #1)   \n",
       "7                            New Moon (Twilight, #2)   \n",
       "8                             Eclipse (Twilight, #3)   \n",
       "9                       Breaking Dawn (Twilight, #4)   \n",
       "\n",
       "                                    recommendation 1  \\\n",
       "0  Being Me With OCD: How I Learned to Obsess Les...   \n",
       "1                    Peggy Sue Contra Los Invisibles   \n",
       "2                               Katso eteesi, Lotta!   \n",
       "3  Alice's adventures in Wonderland ; and, Throug...   \n",
       "4                      Ondine (Ondine Quartet, #0.5)   \n",
       "5                               Katso eteesi, Lotta!   \n",
       "6                                       Kiki and Roo   \n",
       "7                          Fear Hall: The Conclusion   \n",
       "8                          Insurgent (Divergent, #2)   \n",
       "9                                             Sun Up   \n",
       "\n",
       "                                    recommendation 2  \\\n",
       "0                          Everwild (Skinjacker, #2)   \n",
       "1     Supercomputer (Choose Your Own Adventure, #39)   \n",
       "2                     The tale of Jemima puddle-duck   \n",
       "3                      The Aeneid for Boys and Girls   \n",
       "4                    Ölmem Gerekirse (Revenants, #3)   \n",
       "5                      Twinkle, Twinkle, Little Star   \n",
       "6                               Bloß nicht blinzeln!   \n",
       "7                                         Sit Still!   \n",
       "8  Half bad - Det mörka ödet (The Half Bad Trilog...   \n",
       "9  The Mystery of the Burnt Cottage (The Five Fin...   \n",
       "\n",
       "                                    recommendation 3  \\\n",
       "0                 Brave the Betrayal (Everworld, #8)   \n",
       "1  Charlie Bone und das Geheimnis der sprechenden...   \n",
       "2                      Sjælefanger (Riley Bloom, #1)   \n",
       "3                 Brave the Betrayal (Everworld, #8)   \n",
       "4                                  Itsy Bitsy Spider   \n",
       "5                                  Itsy Bitsy Spider   \n",
       "6                            The Princess and Curdie   \n",
       "7  Einstein the Class Hamster and the Very Real G...   \n",
       "8                                         The Island   \n",
       "9                                              Woof!   \n",
       "\n",
       "                       recommendation 4  \\\n",
       "0  Catching Fire (The Hunger Games, #2)   \n",
       "1               New Moon (Twilight, #2)   \n",
       "2  Catching Fire (The Hunger Games, #2)   \n",
       "3     Mockingjay (The Hunger Games, #3)   \n",
       "4                    Looking for Alaska   \n",
       "5  Catching Fire (The Hunger Games, #2)   \n",
       "6             Insurgent (Divergent, #2)   \n",
       "7                Eclipse (Twilight, #3)   \n",
       "8               New Moon (Twilight, #2)   \n",
       "9                Eclipse (Twilight, #3)   \n",
       "\n",
       "                          recommendation 5                   recommendation 6  \n",
       "0        Mockingjay (The Hunger Games, #3)            Twilight (Twilight, #1)  \n",
       "1                   Eclipse (Twilight, #3)       Breaking Dawn (Twilight, #4)  \n",
       "2  The Hunger Games (The Hunger Games, #1)  Mockingjay (The Hunger Games, #3)  \n",
       "3  The Hunger Games (The Hunger Games, #1)          Insurgent (Divergent, #2)  \n",
       "4  The Hunger Games (The Hunger Games, #1)          Divergent (Divergent, #1)  \n",
       "5  The Hunger Games (The Hunger Games, #1)          Insurgent (Divergent, #2)  \n",
       "6  The Hunger Games (The Hunger Games, #1)          Allegiant (Divergent, #3)  \n",
       "7             Breaking Dawn (Twilight, #4)            Twilight (Twilight, #1)  \n",
       "8             Breaking Dawn (Twilight, #4)            Twilight (Twilight, #1)  \n",
       "9                  New Moon (Twilight, #2)            Twilight (Twilight, #1)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate_recommender(test_books, books = books, authors = authors):\n",
    "    records = []\n",
    "    \n",
    "    for book_id in test_books:\n",
    "        try:\n",
    "            test_title = books.loc[books['book_id'] == book_id, 'title'].values[0]\n",
    "        except IndexError:\n",
    "            print(f\"Book ID {book_id} not found in books dataframe.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            recs = get_recommendations(book_id, books=books, authors=authors)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating recommendations for book_id {book_id}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Extract the titles of the top 6 recommendations\n",
    "        top_titles = [rec['title'] for rec in recs[:6]]\n",
    "        \n",
    "        # Pad with None if fewer than 6 recommendations\n",
    "        while len(top_titles) < 6:\n",
    "            top_titles.append(None)\n",
    "        \n",
    "        row = {\n",
    "            'test_book_title': test_title,\n",
    "            'recommendation 1': top_titles[0],\n",
    "            'recommendation 2': top_titles[1],\n",
    "            'recommendation 3': top_titles[2],\n",
    "            'recommendation 4': top_titles[3],\n",
    "            'recommendation 5': top_titles[4],\n",
    "            'recommendation 6': top_titles[5],\n",
    "        }\n",
    "\n",
    "        records.append(row)\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "test_recommendations = evaluate_recommender(test_books)\n",
    "display(test_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6a76c8-3d18-43ff-b09c-984a6e1877c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

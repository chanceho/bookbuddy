{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78a2f407-030f-42ba-8b3b-add892a607d6",
   "metadata": {},
   "source": [
    "# Recommendation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c2c5f-d2ef-40d9-b530-1b6888124c6e",
   "metadata": {},
   "source": [
    "*This Jupyter Notebook imports and combines the two datasets (for young adult and children), performs exploratory data analysis, and generates the output for the Recommendation using both collaborative and content-based filtering.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbe788f-a096-4dfb-91e4-32f77e8ff5f9",
   "metadata": {},
   "source": [
    "# Data Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e298ff-6725-42fd-be73-622d2cac1272",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Importing & Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05453a41-a2c6-4a4d-a658-17d5f0e823fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas\n",
    "! pip install scikit-learn\n",
    "! pip install nltk\n",
    "! pip install spacy\n",
    "! python -m spacy download en_core_web_sm\n",
    "! pip install sentence-transformers\n",
    "! pip install requests jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ebb4f-9f79-4148-ba7f-927359b8a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import sklearn\n",
    "import nltk\n",
    "import spacy\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from nltk import pos_tag\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d44fbd0-5263-4d90-b281-d2e56cf79bba",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01acaf0a-4e96-4628-8a87-72466276a34d",
   "metadata": {},
   "source": [
    "From goodreads, we are able to download three sets of data.\n",
    "\n",
    "1. The `books` dataset outlines associated metadata to a specific book. Things of interest here would be `book_id`, `title`, `average_rating`, `ratings_count`,`description`, `num_pages`, `popular_shelves`,`image_url`,`authors`.\n",
    "2. The `reviews` dataset consists of text reviews that may or may not be added after a rating. As the test reviews do not seem to be useful at this time, we will leave it out. We can consider the data here to scrape for genre.\n",
    "3. The `interactions` dataset indicates whether or not a specific user has read and rated a specific book. It consists of columns `user_id`, `book_id`, `is_read` and `ratings`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0eb1cf-21f2-4d88-bd06-02575b11d476",
   "metadata": {},
   "source": [
    "We have done so for two different age categories (`children` and `young_adult`), and will combine them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec0cd8-182b-493f-b650-9a21d40f1e9a",
   "metadata": {},
   "source": [
    "### Importing and Filtering Books Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321976c3-8bf3-45b6-9938-1a09649ef16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_loading = time.time()\n",
    "columns_of_interest = ['book_id', 'title', 'average_rating', 'ratings_count','description', 'num_pages', 'popular_shelves','image_url','authors']\n",
    "json_files = ['goodreads_books_children.json', 'goodreads_books_young_adult.json']\n",
    "data = []\n",
    "\n",
    "for json_file in json_files:\n",
    "    with open(json_file, 'r') as file:\n",
    "        for line in file:\n",
    "            record = json.loads(line)\n",
    "            filtered_record = {key:record[key] for key in columns_of_interest}\n",
    "            data.append(filtered_record)\n",
    "\n",
    "books = pd.DataFrame(data)\n",
    "books['description_length'] = books['description'].apply(len)\n",
    "books = books[books['description_length'] != 0] #filtering empty descriptions\n",
    "books = books.drop('description_length', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6274b98f-0d36-484b-b944-cd4322072be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_of_interest = ['author_id', 'name']\n",
    "json_files = ['goodreads_book_authors.json']\n",
    "data = []\n",
    "\n",
    "for json_file in json_files:\n",
    "    with open(json_file, 'r') as file:\n",
    "        for line in file:\n",
    "            record = json.loads(line)\n",
    "            filtered_record = {key:record[key] for key in columns_of_interest}\n",
    "            data.append(filtered_record)\n",
    "            \n",
    "authors = pd.DataFrame(data)\n",
    "\n",
    "def get_name(author_id, authors = authors):\n",
    "    # Check if the author_id exists in the DataFrame\n",
    "    if author_id in authors['author_id'].values:\n",
    "        # Return the name corresponding to the author_id\n",
    "        return authors.loc[authors['author_id'] == author_id, 'name'].values[0]\n",
    "    else:\n",
    "        return None\n",
    "end_loading = time.time()\n",
    "duration2 = (end_loading - start_loading)/60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dc7fe3-da55-419b-bae6-19171179f210",
   "metadata": {},
   "source": [
    "### Importing and Filtering Interactions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0426a95-59e6-4cf8-8c64-f9f9db1a82d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_loading = time.time()\n",
    "columns_of_interest = ['user_id','book_id','is_read','rating']\n",
    "json_files = ['goodreads_interactions_children.json', 'goodreads_interactions_young_adult.json']\n",
    "data = []\n",
    "\n",
    "for json_file in json_files:\n",
    "    with open(json_file, 'r') as file:\n",
    "        for line in file:\n",
    "            record = json.loads(line)\n",
    "            filtered_record = {key:record[key] for key in columns_of_interest}\n",
    "            data.append(filtered_record)\n",
    "            \n",
    "interactions = pd.DataFrame(data)\n",
    "interactions = interactions[interactions['is_read'] != 0] #removing ratings by people who have not read the book\n",
    "\n",
    "end_loading = time.time()\n",
    "duration1 = (end_loading - start_loading)/60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356021e4-ff96-4676-bcda-3eaae72ae93b",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ec713d-a2f3-4dc2-b74a-baf81633d018",
   "metadata": {},
   "source": [
    "There are a few ways we can consider hybridizing the approaches. We will now do the ensemble method, which generates two separate recommendation lists and then takes the intersection. The code below should generate 3 random books from the data, which will be used as a test set.\n",
    "\n",
    "Other methods we could consider include (1) weighted hybrid, where a content-based score and a collaborative filtering score is calculated and subsequently combined with a weighted average, or (2) switching hybrid, where content-based filtering is used when the user is new, or when a book has very few ratings, and collaborative filtering is used when a user / book has sufficient history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e68a4c2-e09e-4ac2-b232-a48f4b15c34c",
   "metadata": {},
   "source": [
    "## Generating Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d47506-f8d9-4212-aef2-ac269d654984",
   "metadata": {},
   "source": [
    "### Content-Based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36ccb43-6ba0-4bda-8370-2a085071aa46",
   "metadata": {},
   "source": [
    "For Content-Based Filtering, we use **TF-IDF** and **Cosine Similarity** as our core algorithms. \n",
    "\n",
    "**Term Frequency-Inverse Document Frequency (TF-IDF)** uses NLP to identify important words in the `description` attribute of the selected book by evaluating how frequently they appear, relative to the descriptions of all other books in the dataset. Once this is done, we sort the books by how similar they are using **cosine similarity**, which measures the angle between the two vectors (books). If they have a small angle, the books have similar `description` and is thus considered to be similar in content.\n",
    "\n",
    "A better way to imagine this would be if Book A and Book B both have descriptions that talk about \"magic\", \"spells\" and \"wizards\", they would have similar TF-IDF vectors, and thus high cosine similarity scores.\n",
    "\n",
    "We have also wrote two algorithms to detect genre and suitable age ranges, to identify books in the same genre and similar age ranges. Books in the same genre targeted at a similar age range would enjoy a boost in their similarity scores. The details of detecting genre and detecting age ranges are stipulated below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40001584-1909-4e19-b962-050f2bde4223",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Detecting Genre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e6efe3-9d1c-4db6-ba29-ee7b653dac7c",
   "metadata": {},
   "source": [
    "To detect genre, we first:\n",
    "\n",
    "1. Extract genres from user-assigned shelves (most reliable signal)\n",
    "2. Apply multiple NLP techniques to analyze book description and title. This includes (1) TF-IDF analysis with genre-specific vocabulary and (2) Named entity recognition to identify genre-related entities\n",
    "3. Combine all signals with appropriate weights (shelf data > NLP)\n",
    "4. Return top genres that meet minimum confidence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212dcb10-2e4b-4731-b6f8-dc66104f30df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_book_genre_with_advanced_nlp(book_data, genre_classifier=None, min_confidence=3, exclude_shelves=None):\n",
    "\n",
    "    if exclude_shelves is None:\n",
    "        exclude_shelves = get_default_excluded_shelves()\n",
    "\n",
    "    # Extract genres from structured shelf data\n",
    "    shelf_genres = extract_genres_from_shelves(book_data, get_genre_map(), exclude_shelves)\n",
    "    title = str(book_data.get('title', ''))\n",
    "    description = str(book_data.get('description', ''))\n",
    "\n",
    "    nlp_genres = {}\n",
    "    # Only perform NLP analysis if we have enough text\n",
    "    if len (shelf_genres) < 4 & len(description) > 500:\n",
    "        nlp_genres.update(analyze_with_tfidf(title, description))\n",
    "        nlp_genres.update(extract_named_entities(title, description))\n",
    "\n",
    "\n",
    "    # Combine all signals and apply minimum confidence threshold\n",
    "    final_genres = combine_all_genre_signals(shelf_genres, nlp_genres, min_confidence)\n",
    "    return final_genres[:3]\n",
    "\n",
    "def get_default_excluded_shelves():\n",
    "    #removes shelf names that aren't useful for genre classification\n",
    "    return {\n",
    "        'to-read', 'currently-reading', 'owned', 'default', \n",
    "        'favorites', 'books-i-own', 'ebook', 'kindle', \n",
    "        'library', 'audiobook', 'owned-books', 'to-buy', \n",
    "        'calibre', 're-read', 'unread', 'favourites', 'my-books'\n",
    "    }\n",
    "\n",
    "def get_genre_map():\n",
    "    #Dictionary mapping shelf keywords to standardized genre names\n",
    "    return {\n",
    "        'fantasy': 'Fantasy',\n",
    "        'sci-fi': 'Science Fiction',\n",
    "        'science-fiction': 'Science Fiction',\n",
    "        'mystery': 'Mystery/Thriller',\n",
    "        'thriller': 'Mystery/Thriller',\n",
    "        'romance': 'Romance',\n",
    "        'historical': 'Historical Fiction',\n",
    "        'history': 'History',\n",
    "        'horror': 'Horror',\n",
    "        'young-adult': 'Young Adult',\n",
    "        'ya': 'Young Adult',\n",
    "        'childrens': 'Children\\'s',\n",
    "        'children': 'Children\\'s',\n",
    "        'kids': 'Children\\'s',\n",
    "        'dystopian': 'Dystopian',\n",
    "        'classic': 'Classics',\n",
    "        'classics': 'Classics',\n",
    "        'biography': 'Biography/Memoir',\n",
    "        'memoir': 'Biography/Memoir',\n",
    "        'autobiography': 'Biography/Memoir',\n",
    "        'self-help': 'Self Help',\n",
    "        'business': 'Business',\n",
    "        'philosophy': 'Philosophy',\n",
    "        'psychology': 'Psychology',\n",
    "        'science': 'Science',\n",
    "        'poetry': 'Poetry',\n",
    "        'comic': 'Comics/Graphic Novels',\n",
    "        'graphic-novel': 'Comics/Graphic Novels',\n",
    "        'manga': 'Manga',\n",
    "        'cooking': 'Cooking/Food',\n",
    "        'cookbook': 'Cooking/Food',\n",
    "        'food': 'Cooking/Food',\n",
    "        'travel': 'Travel',\n",
    "        'religion': 'Religion/Spirituality',\n",
    "        'spirituality': 'Religion/Spirituality',\n",
    "        'art': 'Art/Photography',\n",
    "        'photography': 'Art/Photography',\n",
    "        'reference': 'Reference',\n",
    "        'textbook': 'Textbook/Education',\n",
    "        'education': 'Textbook/Education'\n",
    "    }\n",
    "\n",
    "def extract_genres_from_shelves(book_data, genre_map, exclude_shelves):\n",
    "    \n",
    "    #Extract genre information from book's popular shelves data by using shelf counts as confidence scores (more users shelving = higher confidence)\n",
    "    \n",
    "    shelf_genres = {}\n",
    "    \n",
    "    popular_shelves = book_data.get('popular_shelves', [])\n",
    "    if isinstance(popular_shelves, list) and popular_shelves:\n",
    "        for shelf in popular_shelves:\n",
    "            shelf_name = shelf.get('name', '').strip().lower()\n",
    "            shelf_count = int(shelf.get('count', 0))\n",
    "            \n",
    "            if shelf_name in exclude_shelves:\n",
    "                continue\n",
    "            \n",
    "            for keyword, genre_name in genre_map.items():\n",
    "                if keyword in shelf_name:\n",
    "                    if genre_name in shelf_genres:\n",
    "                        shelf_genres[genre_name] += shelf_count\n",
    "                    else:\n",
    "                        shelf_genres[genre_name] = shelf_count\n",
    "                    break\n",
    "    \n",
    "    return shelf_genres\n",
    "\n",
    "def preprocess_text(text, lemmatize=True):\n",
    "    \"\"\"\n",
    "    Preprocess text by removing special characters, lemmatizing, etc. We convert text to lowercase, remove URLs and HTML tags, remove non-alphabetic \n",
    "    characters, normalize whitespace and optionally lemmatize words (reduce to base form)\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        word_list = nltk.word_tokenize(text)\n",
    "        text = ' '.join([lemmatizer.lemmatize(word) for word in word_list])\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def load_nlp_models():\n",
    "    \n",
    "    #Load spaCy models required for named entity recognition\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    except:\n",
    "        try:\n",
    "            spacy.cli.download(\"en_core_web_sm\")\n",
    "            nlp = spacy.load(\"en_core_web_sm\")\n",
    "        except:\n",
    "            nlp = None\n",
    "            \n",
    "    return nlp\n",
    "\n",
    "\n",
    "def analyze_with_tfidf(title, description):\n",
    "    \"\"\"\n",
    "    Analyze book text using TF-IDF comparison against genre-specific vocabulary.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Define genre-specific keyword sets\n",
    "    2. Preprocess the book text (title + description)\n",
    "    3. Create TF-IDF vectors for genre keywords and book text\n",
    "    4. Calculate cosine similarity between book vector and each genre vector\n",
    "    5. Convert similarities to confidence scores and return top matches\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Define genre keyword sets\n",
    "        genre_keywords = {\n",
    "            'Fantasy': 'magic wizard dragon elf quest sword magical kingdom witch sorcery myth fantasy',\n",
    "            'Science Fiction': 'space alien future technology robot dystopian sci-fi futuristic planet spacecraft',\n",
    "            'Mystery/Thriller': 'murder detective crime case investigation killer suspense clue mystery conspiracy',\n",
    "            'Romance': 'love relationship passion romantic heart affair marriage emotion desire dating romance',\n",
    "            'Historical Fiction': 'century historical period king queen ancient war empire era medieval history',\n",
    "            'Horror': 'fear terror ghost scary monster supernatural haunt nightmare blood evil dark horror',\n",
    "            'Young Adult': 'teen school young coming-of-age adolescent teenage youth friendship high-school',\n",
    "            'Children\\'s': 'child kid young picture-book learning bedtime simple adventure colorful illustrated',\n",
    "            'Biography/Memoir': 'life autobiography personal real journey memoir experience story true figure',\n",
    "            'Self Help': 'improve success happiness guide advice life motivation habit inspiration growth',\n",
    "            'Business': 'market company entrepreneur success management leadership strategy finance career investment',\n",
    "            'Dystopian': 'dystopia future society control survival oppression rebellion totalitarian apocalyptic regime'\n",
    "        }\n",
    "        \n",
    "        # Preprocess text\n",
    "        processed_text = preprocess_text(f\"{title} {description}\")\n",
    "        \n",
    "        # Create TF-IDF vectorizer\n",
    "        vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "        \n",
    "        # Create corpus with genre keywords and the book text\n",
    "        corpus = list(genre_keywords.values())\n",
    "        corpus.append(processed_text)\n",
    "        \n",
    "        # Calculate TF-IDF\n",
    "        tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "        \n",
    "        # Calculate similarity between book and each genre\n",
    "        last_row_index = tfidf_matrix.shape[0] - 1\n",
    "        similarities = cosine_similarity(tfidf_matrix[last_row_index], tfidf_matrix[:-1])[0]\n",
    "        \n",
    "        # Map similarities to genres\n",
    "        genres = {}\n",
    "        for i, genre in enumerate(genre_keywords.keys()):\n",
    "            # Convert similarity scores to a more intuitive range (0-10)\n",
    "            score = int(similarities[i] * 10)\n",
    "            if score > 3:  # Only consider reasonable matches\n",
    "                genres[genre] = score\n",
    "                \n",
    "        return genres\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def extract_named_entities(title, description):\n",
    "    #Extract named entities from book text, use spaCy NLP model to process text and extract named entities before and mapping them to potential genres.\n",
    "    \n",
    "    try:\n",
    "        nlp = load_nlp_models()\n",
    "        if not nlp:\n",
    "            return {}\n",
    "            \n",
    "        # Process text with spaCy\n",
    "        doc = nlp(f\"{title} {description}\")\n",
    "        \n",
    "        # Extract entities\n",
    "        entities = [ent.text.lower() for ent in doc.ents]\n",
    "        \n",
    "        # Define entity-genre associations\n",
    "        entity_genre_map = {\n",
    "            'fantasy': ['magic', 'wizard', 'dragon', 'elf', 'fairy', 'kingdom', 'quest', 'sorcerer'],\n",
    "            'science fiction': ['space', 'planet', 'alien', 'robot', 'future', 'technology'],\n",
    "            'historical fiction': ['century', 'king', 'queen', 'empire', 'war', 'battle', 'medieval', 'ancient'],\n",
    "            'biography': ['life', 'biography', 'autobiography', 'memoir', 'president', 'politician', 'artist'],\n",
    "            'science': ['research', 'experiment', 'theory', 'physics', 'biology', 'chemistry', 'scientist'],\n",
    "            'religion': ['god', 'church', 'bible', 'faith', 'spiritual', 'religion', 'prayer']\n",
    "        }\n",
    "        \n",
    "        # Find genres based on entities\n",
    "        genres = {}\n",
    "        for entity in entities:\n",
    "            for genre, keywords in entity_genre_map.items():\n",
    "                if any(keyword in entity for keyword in keywords):\n",
    "                    standardized_genre = standardize_genre(genre)\n",
    "                    genres[standardized_genre] = genres.get(standardized_genre, 0) + 1\n",
    "                    \n",
    "        return genres\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "def standardize_genre(genre):\n",
    "    #Standardize genre names to a consistent format.\n",
    "    \n",
    "    genre_map = {\n",
    "        'fantasy': 'Fantasy',\n",
    "        'science fiction': 'Science Fiction',\n",
    "        'historical fiction': 'Historical Fiction',\n",
    "        'biography': 'Biography/Memoir',\n",
    "        'science': 'Science',\n",
    "        'religion': 'Religion/Spirituality'\n",
    "    }\n",
    "    return genre_map.get(genre.lower(), genre.title())\n",
    "\n",
    "\n",
    "def combine_all_genre_signals(shelf_genres, nlp_genres, min_confidence):\n",
    "    #Combine genre signals from different sources with appropriate weights.\n",
    "    \n",
    "    combined_scores = Counter()\n",
    "    \n",
    "    # Add shelf genres with highest weight (explicit human categorization)\n",
    "    for genre, score in shelf_genres.items():\n",
    "        combined_scores[genre] += score * 3\n",
    "    \n",
    "    # Add NLP-detected genres with medium weight\n",
    "    for genre, score in nlp_genres.items():\n",
    "        combined_scores[genre] += score * 2\n",
    "    \n",
    "    # Sort by final score\n",
    "    sorted_genres = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Filter by minimum confidence\n",
    "    final_genres = [genre for genre, score in sorted_genres if score >= min_confidence]\n",
    "    \n",
    "    # Fallback if no confident genres\n",
    "    if not final_genres and sorted_genres:\n",
    "        final_genres = [sorted_genres[0][0]]\n",
    "    \n",
    "    return final_genres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83346f8-7b97-4216-b398-83102a956bac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Detecting Suitable Age Range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeda18c-34ee-4774-9cc7-c3f6fd61601b",
   "metadata": {},
   "source": [
    "To detect age ranges, we first:\n",
    "\n",
    "1. Extract age ranges from `popular_shelves` (most reliable signal)\n",
    "2. Failing that, we estimate the targeted age range by approximating the difficulty of the book (1) using the number of pages `num_pages`, (2) complexity of language in the book's `title` and `description` and (3) analysis of possible themes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32103eca-8712-4eda-baea-a969fc7a3d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_age_range(book_data):\n",
    "    if book_data.get('popular_shelves') is None:\n",
    "        book_data['popular_shelves'] = []\n",
    "    \n",
    "    title = str(book_data.get('title', ''))\n",
    "    description = str(book_data.get('description', ''))\n",
    "    num_pages = book_data['num_pages']\n",
    "\n",
    "    try:\n",
    "        num_pages = int(book_data['num_pages'])\n",
    "    except (KeyError, ValueError, TypeError):\n",
    "        num_pages = 0\n",
    "    \n",
    "    age_scores = {\n",
    "        '0-5': 0,\n",
    "        '5-10': 0,\n",
    "        '10-15': 0,\n",
    "        '15+': 0\n",
    "    }\n",
    "    \n",
    "    title_lower = title.lower()\n",
    "    description_lower = description.lower()\n",
    "    \n",
    "    # Advanced NLP analysis of title and description\n",
    "    title_complexity = analyze_text_complexity(title)\n",
    "    desc_complexity = analyze_text_complexity(description)\n",
    "    \n",
    "    # Apply title complexity to age scores\n",
    "    if title_complexity < 0.3:\n",
    "        age_scores['0-5'] += 10\n",
    "        age_scores['5-10'] += 5\n",
    "    elif title_complexity < 0.5:\n",
    "        age_scores['5-10'] += 8\n",
    "        age_scores['0-5'] += 4\n",
    "    elif title_complexity < 0.7:\n",
    "        age_scores['10-15'] += 8\n",
    "        age_scores['5-10'] += 4\n",
    "    else:\n",
    "        age_scores['15+'] += 8\n",
    "        age_scores['10-15'] += 4\n",
    "    \n",
    "    # Apply description complexity to age scores\n",
    "    if desc_complexity < 0.3:\n",
    "        age_scores['0-5'] += 12\n",
    "        age_scores['5-10'] += 6\n",
    "    elif desc_complexity < 0.5:\n",
    "        age_scores['5-10'] += 10\n",
    "        age_scores['0-5'] += 5\n",
    "    elif desc_complexity < 0.7:\n",
    "        age_scores['10-15'] += 10\n",
    "        age_scores['5-10'] += 5\n",
    "    else:\n",
    "        age_scores['15+'] += 12\n",
    "        age_scores['10-15'] += 6\n",
    "    \n",
    "    # POS tag patterns analysis for age appropriateness\n",
    "    pos_patterns = analyze_pos_patterns(description)\n",
    "    for age_range, score in pos_patterns.items():\n",
    "        age_scores[age_range] += score\n",
    "    \n",
    "    board_book_terms = ['board book', 'bedtime', 'goodnight', 'naptime', 'toddler', 'baby', \n",
    "                        'alphabet', 'counting', 'colors', 'shapes', 'lullaby', 'nursery']\n",
    "    \n",
    "    if any(term in title_lower or term in description_lower for term in board_book_terms):\n",
    "        age_scores['0-5'] += 12\n",
    "        age_scores['5-10'] -= 5\n",
    "    \n",
    "    early_reader_terms = ['early reader', 'beginning reader', 'learn to read', 'level reader',\n",
    "                          'first reader', 'step into reading', 'i can read', 'reading level']\n",
    "    \n",
    "    if any(term in title_lower or term in description_lower for term in early_reader_terms):\n",
    "        age_scores['5-10'] += 12\n",
    "        age_scores['0-5'] -= 2\n",
    "    \n",
    "    grade_terms = {\n",
    "        '0-5': ['preschool', 'pre-k', 'kindergarten'],\n",
    "        '5-10': ['grade 1', 'grade 2', 'grade 3', 'grade 4', 'first grade', 'second grade', \n",
    "                'third grade', 'fourth grade', 'fifth grade', 'elementary'],\n",
    "        '10-15': ['grade 5', 'grade 6', 'grade 7', 'grade 8', 'middle school', 'middle-grade', \n",
    "                 'middle grade', 'tween'],\n",
    "        '15+': ['grade 9', 'grade 10', 'grade 11', 'grade 12', 'high school', 'teen', 'young adult',\n",
    "               'ya', 'college', 'university']\n",
    "    }\n",
    "    \n",
    "    for age_range, terms in grade_terms.items():\n",
    "        if any(term in title_lower or term in description_lower for term in terms):\n",
    "            age_scores[age_range] += 10\n",
    "    \n",
    "    if num_pages <= 32:\n",
    "        age_scores['0-5'] += 15\n",
    "        age_scores['5-10'] -= 3\n",
    "    elif 33 <= num_pages <= 48:\n",
    "        age_scores['0-5'] += 10\n",
    "        age_scores['5-10'] += 5\n",
    "    elif 49 <= num_pages <= 80:\n",
    "        age_scores['5-10'] += 12\n",
    "        age_scores['0-5'] -= 2\n",
    "    elif 81 <= num_pages <= 120:\n",
    "        age_scores['5-10'] += 8\n",
    "        age_scores['10-15'] += 4\n",
    "    elif 121 <= num_pages <= 200:\n",
    "        age_scores['10-15'] += 8\n",
    "        age_scores['5-10'] += 4\n",
    "    elif 201 <= num_pages <= 350:\n",
    "        age_scores['10-15'] += 10\n",
    "        age_scores['15+'] += 5\n",
    "    elif num_pages > 350:\n",
    "        age_scores['15+'] += 12\n",
    "        age_scores['10-15'] += 6\n",
    "\n",
    "    # Content theme analysis with increased weight for theme matches\n",
    "    content_themes = analyze_text_themes(title_lower, description_lower)\n",
    "    for age_range, score in content_themes.items():\n",
    "        age_scores[age_range] += score * 1.5\n",
    "    \n",
    "    # Shelf analysis\n",
    "    shelves = book_data.get('popular_shelves', [])\n",
    "    shelf_age_indicators = analyze_shelves_for_age(shelves)\n",
    "    for age_range, score in shelf_age_indicators.items():\n",
    "        age_scores[age_range] += score\n",
    "    \n",
    "    max_score = max(age_scores.values())\n",
    "    final_age_range = max(age_scores.items(), key=lambda x: x[1])[0]\n",
    "    \n",
    "    return final_age_range\n",
    "\n",
    "def analyze_text_complexity(text):\n",
    "    if not text or len(text) < 5:\n",
    "        return 0.5\n",
    "    \n",
    "    try:\n",
    "        sentences = sent_tokenize(text)\n",
    "        words = word_tokenize(text)\n",
    "        \n",
    "        if not sentences or not words:\n",
    "            return 0.5\n",
    "        \n",
    "        avg_sentence_length = len(words) / max(1, len(sentences))\n",
    "        avg_word_length = sum(len(word) for word in words if word.isalpha()) / max(1, len([w for w in words if w.isalpha()]))\n",
    "        \n",
    "        # Calculate lexical diversity (larger vocabulary suggests more complex text)\n",
    "        unique_words = len(set(word.lower() for word in words if word.isalpha()))\n",
    "        lexical_diversity = unique_words / max(1, len([w for w in words if w.isalpha()]))\n",
    "        \n",
    "        # Calculate percentage of complex words (words with 3+ syllables)\n",
    "        complex_words = sum(1 for word in words if word.isalpha() and textstat.syllable_count(word) >= 3)\n",
    "        complex_words_pct = complex_words / max(1, len([w for w in words if w.isalpha()]))\n",
    "        \n",
    "        # Weighted complexity score\n",
    "        complexity_score = (\n",
    "            (avg_sentence_length / 25) * 0.3 + \n",
    "            (avg_word_length / 7) * 0.2 + \n",
    "            lexical_diversity * 0.25 + \n",
    "            complex_words_pct * 0.25\n",
    "        )\n",
    "        \n",
    "        return min(1.0, complexity_score)\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "def analyze_pos_patterns(text):\n",
    "    try:\n",
    "        age_patterns = {\n",
    "            '0-5': 0,\n",
    "            '5-10': 0,\n",
    "            '10-15': 0,\n",
    "            '15+': 0\n",
    "        }\n",
    "        \n",
    "        # Get POS tags\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        tagged = pos_tag(tokens)\n",
    "        \n",
    "        # Count parts of speech\n",
    "        pos_counts = Counter(tag for word, tag in tagged)\n",
    "        total_tokens = len(tagged)\n",
    "        \n",
    "        if total_tokens == 0:\n",
    "            return age_patterns\n",
    "        \n",
    "        # Simple sentence structure (mainly nouns and verbs) - for young children\n",
    "        simple_structure = (pos_counts.get('NN', 0) + pos_counts.get('NNS', 0) + \n",
    "                           pos_counts.get('VB', 0) + pos_counts.get('VBZ', 0) + \n",
    "                           pos_counts.get('VBP', 0)) / total_tokens\n",
    "        \n",
    "        # Complex sentence markers (conjunctions, relative pronouns, etc.)\n",
    "        complex_markers = (pos_counts.get('IN', 0) + pos_counts.get('WDT', 0) + \n",
    "                          pos_counts.get('WP', 0) + pos_counts.get('WRB', 0)) / total_tokens\n",
    "        \n",
    "        # Advanced language features (adjectives, adverbs, etc.)\n",
    "        advanced_features = (pos_counts.get('JJ', 0) + pos_counts.get('JJR', 0) + \n",
    "                            pos_counts.get('JJS', 0) + pos_counts.get('RB', 0) + \n",
    "                            pos_counts.get('RBR', 0) + pos_counts.get('RBS', 0)) / total_tokens\n",
    "        \n",
    "        # Score assignment based on POS patterns\n",
    "        if simple_structure > 0.6 and complex_markers < 0.1:\n",
    "            age_patterns['0-5'] += 8\n",
    "            age_patterns['5-10'] += 4\n",
    "        elif simple_structure > 0.5 and complex_markers < 0.15:\n",
    "            age_patterns['5-10'] += 7\n",
    "            age_patterns['0-5'] += 3\n",
    "        elif complex_markers > 0.15 and advanced_features > 0.2:\n",
    "            age_patterns['10-15'] += 6\n",
    "            age_patterns['15+'] += 3\n",
    "        elif complex_markers > 0.2 and advanced_features > 0.25:\n",
    "            age_patterns['15+'] += 8\n",
    "            age_patterns['10-15'] += 4\n",
    "        \n",
    "        return age_patterns\n",
    "    except:\n",
    "        return {\n",
    "            '0-5': 0,\n",
    "            '5-10': 0,\n",
    "            '10-15': 0,\n",
    "            '15+': 0\n",
    "        }\n",
    "\n",
    "def analyze_text_themes(title, description):\n",
    "    combined_text = title + \" \" + description\n",
    "    \n",
    "    theme_scores = {\n",
    "        '0-5': 0,\n",
    "        '5-10': 0,\n",
    "        '10-15': 0,\n",
    "        '15+': 0\n",
    "    }\n",
    "    \n",
    "    early_themes = ['sleep', 'bed', 'nap', 'dream', 'moon', 'star', 'night', 'bunny', 'teddy', \n",
    "                    'toy', 'farm', 'animal', 'cat', 'dog', 'duck', 'color', 'zoo', 'mommy', \n",
    "                    'daddy', 'parent', 'bath', 'diaper', 'potty', 'train', 'truck', 'car', \n",
    "                    'alphabet', 'abc', 'number', '123', 'count', 'rhyme']\n",
    "    \n",
    "    elementary_themes = ['school', 'teacher', 'friend', 'adventure', 'fun', 'magic', 'fairy', \n",
    "                        'dragon', 'dinosaur', 'spy', 'detective', 'mystery', 'solve', 'game', \n",
    "                        'play', 'team', 'sport', 'chapter', 'series', 'collect', 'comic', \n",
    "                        'joke', 'funny', 'humor', 'silly', 'prank', 'robot', 'space', 'science']\n",
    "    \n",
    "    middle_themes = ['friend', 'school', 'bully', 'crush', 'team', 'competition', 'journal', \n",
    "                    'diary', 'secret', 'club', 'grow', 'family', 'sibling', 'parent', 'problem', \n",
    "                    'solve', 'quest', 'mission', 'summer', 'camp', 'vacation', 'holiday', \n",
    "                    'fantasy', 'world', 'magic', 'spell', 'creature', 'monster', 'ghost']\n",
    "    \n",
    "    ya_themes = ['love', 'romance', 'relationship', 'kiss', 'boyfriend', 'girlfriend', 'dating', \n",
    "                'death', 'tragedy', 'war', 'battle', 'fight', 'survive', 'future', 'dystopian', \n",
    "                'apocalypse', 'society', 'rebellion', 'government', 'power', 'politics', 'identity', \n",
    "                'struggle', 'college', 'career', 'adult', 'mature', 'violence', 'blood']\n",
    "    \n",
    "    for theme in early_themes:\n",
    "        if theme in combined_text:\n",
    "            theme_scores['0-5'] += 1.5\n",
    "    \n",
    "    for theme in elementary_themes:\n",
    "        if theme in combined_text:\n",
    "            theme_scores['5-10'] += 1.5\n",
    "    \n",
    "    for theme in middle_themes:\n",
    "        if theme in combined_text:\n",
    "            theme_scores['10-15'] += 1.5\n",
    "    \n",
    "    for theme in ya_themes:\n",
    "        if theme in combined_text:\n",
    "            theme_scores['15+'] += 1.5\n",
    "    \n",
    "    return theme_scores\n",
    "\n",
    "def analyze_shelves_for_age(shelves):\n",
    "    shelf_patterns = {\n",
    "        '0-5': ['picture book', 'board book', 'childrens', 'toddler', 'baby', 'preschool', \n",
    "               'bedtime', 'nursery', 'concept book'],\n",
    "        '5-10': ['early reader', 'chapter book', 'childrens', 'kids', 'elementary', 'juvenile', \n",
    "                'easy reader'],\n",
    "        '10-15': ['middle grade', 'middle-grade', 'tween', 'juvenile', 'preteen'],\n",
    "        '15+': ['young adult', 'ya', 'teen', 'high school', 'new adult', 'adult']\n",
    "    }\n",
    "    \n",
    "    shelf_scores = {\n",
    "        '0-5': 0,\n",
    "        '5-10': 0,\n",
    "        '10-15': 0,\n",
    "        '15+': 0\n",
    "    }\n",
    "    \n",
    "    for shelf in shelves:\n",
    "        shelf_name = shelf.get('name', '').lower()\n",
    "        shelf_count = int(shelf.get('count', 0))\n",
    "        \n",
    "        for age_range, patterns in shelf_patterns.items():\n",
    "            if any(pattern in shelf_name for pattern in patterns):\n",
    "                shelf_scores[age_range] += min(12, math.log(shelf_count + 1) * 2)\n",
    "    \n",
    "    return shelf_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee9f08a-9e7c-409c-ae00-d4dbd8810fe5",
   "metadata": {},
   "source": [
    "#### Combining Genre, Age and Description Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bb9f67-96ea-4397-a332-7384fb9bb3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "tfidf = TfidfVectorizer(stop_words = 'english') \n",
    "tfidf_matrix = tfidf.fit_transform(books['description']) #generating TF-IDF matrix\n",
    "end = time.time()\n",
    "duration3 = (end - start)/60\n",
    "\n",
    "def get_content_recommendations (book_id, df = books, tfidf_matrix = tfidf_matrix, top_n = 3):\n",
    "    index = df[df['book_id'] == book_id].index[0]\n",
    "    book_data = df.iloc[index].to_dict()\n",
    "    start = time.time()\n",
    "    target_genres = detect_book_genre_with_advanced_nlp(book_data)\n",
    "    target_age_range = detect_age_range(book_data)\n",
    "    end = time.time()\n",
    "    duration4 = (end - start)/60\n",
    "    print(f'Time for identifying target genre and age range is {duration4:.4f} mins.')\n",
    "    sim_scores = cosine_similarity(tfidf_matrix[index], tfidf_matrix).flatten() #calculating cosine similarity\n",
    "    start = time.time()\n",
    "    for i in range(len(sim_scores)):\n",
    "        if i == index:\n",
    "            continue\n",
    "        book_data_i = df.iloc[i].to_dict()\n",
    "        genres_i = detect_book_genre_with_advanced_nlp(book_data_i)\n",
    "        age_range_i = detect_age_range(book_data_i)\n",
    "\n",
    "        if set(target_genres) & set(genres_i):\n",
    "            sim_scores[i] *= 2\n",
    "        if set(target_age_range) & set(age_range_i):\n",
    "            sim_scores[i] *= 2        \n",
    "        \n",
    "    end = time.time()\n",
    "    duration5 = (end - start)/60\n",
    "    print(f'Time for updating similarity scores is {duration5:.4f} mins. Exiting content recommendations algorithm.')\n",
    "    \n",
    "    top_indices = np.argsort(sim_scores)[::1][1:top_n + 1]\n",
    "\n",
    "    recommendations = df.iloc[top_indices][['book_id']]\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79039930-ecdd-488e-b4aa-85b7752ee15e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855c2797-a584-4165-b923-c952be9e8c85",
   "metadata": {},
   "source": [
    "We will use K-Nearest Neighbours as an algorithm to perform Collaborative Filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f0b03b-ec8c-4a0c-a2e2-c7c02f7dc704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_item_matrix(df): #to create user-item matrix for collaborative filtering\n",
    "    users = interactions['user_id'].nunique()\n",
    "    items = interactions['book_id'].nunique()\n",
    "    \n",
    "    user_mapper = dict(zip(np.unique(interactions['user_id']), list(range(users))))\n",
    "    user_inv_mapper = dict(zip(list(range(users)), np.unique(interactions['user_id'])))\n",
    "    user_index = [user_mapper[i] for i in interactions['user_id']]\n",
    "\n",
    "    \n",
    "    item_mapper = dict(zip(np.unique(interactions['book_id']), list(range(items))))\n",
    "    item_inv_mapper = dict(zip(list(range(items)), np.unique(interactions['book_id'])))\n",
    "    item_index = [item_mapper[i] for i in interactions['book_id']]\n",
    "    \n",
    "    user_item_matrix = csr_matrix((interactions['rating'], (user_index, item_index)), shape = (users, items))\n",
    "    return user_item_matrix, user_mapper, item_mapper, user_inv_mapper, item_inv_mapper\n",
    "\n",
    "user_item_matrix, user_mapper, item_mapper, user_inv_mapper, item_inv_mapper = create_user_item_matrix(interactions)\n",
    "\n",
    "def get_collaborative_recommendations(book_id, books = books, user_item_matrix = user_item_matrix, item_mapper = item_mapper, item_inv_mapper = item_inv_mapper, top_n = 3):\n",
    "    user_item_matrix = user_item_matrix.T\n",
    "    neighbor_ids = []\n",
    "    recommendations = []\n",
    "\n",
    "    item_ind = item_mapper[book_id]\n",
    "    item_vec = user_item_matrix[item_ind]\n",
    "    if isinstance(item_vec, (np.ndarray)):\n",
    "        item_vec = item_vec.reshape(1,-1)\n",
    "\n",
    "    kNN = NearestNeighbors(n_neighbors = top_n + 1, algorithm = \"brute\", metric = \"cosine\") #measuring similarity using K-Nearest-Neighbors\n",
    "    kNN.fit(user_item_matrix)\n",
    "    neighbor = kNN.kneighbors(item_vec, return_distance = False)\n",
    "    for i in range (0, top_n):\n",
    "        n = neighbor.item(i)\n",
    "        neighbor_ids.append(item_inv_mapper[n])\n",
    "    neighbor_ids.pop(0)\n",
    "\n",
    "    for id in neighbor_ids: #retrieving book titles\n",
    "        recommended_books = books.loc[books['book_id'] == id, ['book_id','title']].values[0]\n",
    "        recommendations.append({'book_id': recommended_books[0], 'title': recommended_books[1]})\n",
    "        \n",
    "    recommendations_df = pd.DataFrame(recommendations)\n",
    "    return recommendations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b415c7-b2c8-445a-87c0-238cdc49762b",
   "metadata": {},
   "source": [
    "## Parsing Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ade622-a348-4d1e-9a61-8715594ad37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(book_id, books = books, authors = authors):\n",
    "    start = time.time()\n",
    "    \n",
    "    df1 = get_content_recommendations(book_id)\n",
    "    step1 = time.time()\n",
    "    print(f'Content Recommendations retrieved! Total Time taken was {(step1 - start)/60:.4f} mins.')\n",
    "    df2 = get_collaborative_recommendations(book_id)\n",
    "    print('Collaborative Recommendations retrieved!')\n",
    "    recommendations = pd.concat([df1, df2], ignore_index = True)\n",
    "\n",
    "    result = []\n",
    "    print('Writing to File...')\n",
    "\n",
    "    for book_id in recommendations['book_id']:\n",
    "        book_details = books[books['book_id'] == book_id].iloc[0]\n",
    "        \n",
    "        authors_list = book_details['authors']\n",
    "        author_names = []\n",
    "        for author in authors_list:\n",
    "            author_id = author['author_id']\n",
    "            author_name = get_name(author_id) \n",
    "            author_names.append(author_name)\n",
    "\n",
    "        if len(author_names) > 1:\n",
    "            concat_authors = \" & \".join(author_names)\n",
    "        else:\n",
    "            concat_authors = author_names[0] if author_names else \"Unknown\"\n",
    "\n",
    "        book_metadata = {\n",
    "            \"bookid\": book_details['book_id'],\n",
    "            \"title\": book_details['title'],\n",
    "            \"author\": concat_authors,\n",
    "            \"coverimage\": book_details['image_url']\n",
    "        }\n",
    "\n",
    "        result.append(book_metadata)\n",
    "\n",
    "        with open('recommendations.json', 'w') as f:\n",
    "            json.dump(result, f, indent=4)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0058c569-2070-439b-9635-fca3c2a755a5",
   "metadata": {},
   "source": [
    "## Testing with Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bec8b85-a66f-4c8f-b5e2-5fc7426e4cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Time taken for loading books was {duration1:.4f} mins.')\n",
    "print(f'Time taken for loading interactions was {duration2:.4f} mins.')\n",
    "print(f'Time taken for generating TFIDF matrix was {duration3:.4f} mins.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184a0c83-f597-44ee-af38-22a285ed06b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_set(df, n = 5):\n",
    "    selected_books = df.sample(n)\n",
    "    return selected_books['book_id']\n",
    "test_set = generate_test_set(books)\n",
    "print(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4396a32-0813-4e82-8d3a-dd1465e462f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for book in test_set:\n",
    "    recommendations = get_recommendations(book)\n",
    "    print(recommendations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
